## Modelos de DifusÃ£o em World Foundation Models (WFMs)

 No contexto dos World Foundation Models (WFMs), **ele transforma ruÃ­do em uma simulaÃ§Ã£o de vÃ­deo do mundo.**

Analogia: "Pense em um modelo de difusÃ£o como um artista que comeÃ§a com uma tela cheia de ruÃ­do aleatÃ³rio (como uma "chuva" de TV antiga) e, gradualmente, passo a passo, aprende a remover esse ruÃ­do, revelando uma imagem ou vÃ­deo coerente e significativo."

![figure](https://miro.medium.com/v2/resize:fit:1400/0*Gv-tiaJtJDPcvwu8.png)

### TokenizaÃ§Ã£o de VÃ­deo: Transformando VÃ­deos em "Latentes ContÃ­nuos"

Assim como os modelos autoregressivos, os modelos de difusÃ£o precisam processar vÃ­deos em um formato mais gerenciÃ¡vel para sua operaÃ§Ã£o.

- Tokens ContÃ­nuos: Para modelos de difusÃ£o, os vÃ­deos sÃ£o transformados em embeddings latentes contÃ­nuos (vetores de nÃºmeros decimais). Pense neles como uma representaÃ§Ã£o compacta e fluida do vÃ­deo, em oposiÃ§Ã£o aos "tokens discretos" (nÃºmeros inteiros) usados pelos modelos autoregressivos.

- Cosmos Continuous Tokenizer (Cosmos-1.0-Tokenizer-CV8x8x8): Este Ã© o componente responsÃ¡vel por essa transformaÃ§Ã£o. Ele comprime o vÃ­deo de entrada em uma representaÃ§Ã£o latente de menor dimensÃ£o, preservando a maior parte da informaÃ§Ã£o visual. Este tokenizer possui uma arquitetura de codificador-decodificador que opera no espaÃ§o wavelet para maior compressÃ£o e preservaÃ§Ã£o de informaÃ§Ãµes semÃ¢nticas, alÃ©m de um design causal temporal (a codificaÃ§Ã£o de quadros atuais nÃ£o depende de quadros futuros, crucial para aplicaÃ§Ãµes de IA FÃ­sica).

### FormulaÃ§Ã£o: O Processo de Denoising (RemoÃ§Ã£o de RuÃ­do)

O cerne do modelo de difusÃ£o Ã© o processo iterativo de "denoising" (remoÃ§Ã£o de ruÃ­do).

#### Detalhes da FormulaÃ§Ã£o

|              **Aspecto**              | **DescriÃ§Ã£o**                                                                                                                                                                                                                                                                                                                                                      |
| :-----------------------------------: | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
|       AdiÃ§Ã£o e RemoÃ§Ã£o de RuÃ­do       | Durante o treinamento, **ruÃ­do gaussiano (aleatÃ³rio)** Ã© progressivamente adicionado a um vÃ­deo real. O modelo Ã©, entÃ£o, treinado para inverter esse processo, aprendendo a remover o ruÃ­do em cada etapa para reconstruir o vÃ­deo original a partir de uma versÃ£o ruidosa.<br><br>                                                                                |
|      FunÃ§Ã£o Denoising (D_theta)       | O modelo de difusÃ£o utiliza uma rede neural **D_theta** (chamada "denoiser") treinada para estimar o ruÃ­do presente em uma amostra corrompida (vÃ­deo com ruÃ­do) e, consequentemente, removÃª-lo para chegar Ã  versÃ£o limpa do vÃ­deo.                                                                                                                                |
|            FunÃ§Ã£o de Perda            | O treinamento emprega uma funÃ§Ã£o de perda de **"denoising score matching"** que penaliza a diferenÃ§a entre o ruÃ­do previsto pelo modelo e o ruÃ­do real adicionado. Uma tÃ©cnica de **ponderaÃ§Ã£o baseada em incerteza (mu(sigma))** Ã© utilizada para gerenciar o aprendizado em diferentes nÃ­veis de ruÃ­do, tratando-o como um problema de aprendizado multi-tarefa. |


![figure2](https://miro.medium.com/v2/resize:fit:1400/0*rqhDUmWmJsSquQwP.png)

### Arquitetura do Modelo: Como o Denoising Ã© ConstruÃ­do

A rede D_theta do modelo de difusÃ£o Ã© uma adaptaÃ§Ã£o de uma arquitetura Transformer, otimizada para dados visuais e controle.

#### Componentes ArquitetÃ´nicos Chave

|                **Componente**                 | **DescriÃ§Ã£o**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| :-------------------------------------------: | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
|               PatchificaÃ§Ã£o 3D                | As representaÃ§Ãµes latentes de entrada sÃ£o convertidas em **"patches" (pedaÃ§os cÃºbicos) tridimensionais**, que sÃ£o entÃ£o "achatados" em uma sequÃªncia unidimensional. Isso prepara os dados para serem processados eficientemente pelo Transformer.<br><br>                                                                                                                                                                                                                                                                                                                   |
|        Embeddings Posicionais HÃ­bridos        | Essenciais para a compreensÃ£o espacial e temporal:  <br>â€¢ **Rotary Position Embedding (RoPE) Fatorado em 3D**: Ajuda o modelo a entender as posiÃ§Ãµes relativas dos tokens nas dimensÃµes temporal, de altura e de largura, permitindo a geraÃ§Ã£o de vÃ­deos de tamanhos e duraÃ§Ãµes arbitrÃ¡rias, compatÃ­vel com diferentes taxas de quadros (FPS).  <br>â€¢ **Embedding Posicional Absoluto (AprendÃ­vel)**: Um embedding adicional usado em cada bloco Transformer que, combinado com RoPE, melhora o desempenho, reduz a perda de treinamento e minimiza artefatos de "morphing". |
| Cross-Attention para Condicionamento de Texto | Camadas integradas que permitem ao modelo gerar vÃ­deos com base em descriÃ§Ãµes de texto, incorporando informaÃ§Ãµes de **embeddings de texto** (gerados pelo **T5-XXL**) no processo de denoising.                                                                                                                                                                                                                                                                                                                                                                              |
|           QK-NormalizaÃ§Ã£o (QKNorm)            | Normaliza os vetores de "query" (Q) e "key" (K) antes da operaÃ§Ã£o de atenÃ§Ã£o, o que aumenta a **estabilidade do treinamento**, especialmente nas fases iniciais, prevenindo a saturaÃ§Ã£o da atenÃ§Ã£o.                                                                                                                                                                                                                                                                                                                                                                          |
|                  AdaLN-LoRA                   | Uma otimizaÃ§Ã£o arquitetÃ´nica que **reduz significativamente a contagem de parÃ¢metros** (ex: 36% para o modelo de 7B parÃ¢metros) sem comprometer o desempenho, tornando o modelo mais eficiente em termos de memÃ³ria e computaÃ§Ã£o.                                                                                                                                                                                                                                                                                                                                            |

### EstratÃ©gia de Treinamento: Como o Modelo Aprende a "Pintar"

Os modelos de difusÃ£o sÃ£o treinados em vÃ¡rias etapas para otimizar seu desempenho e generalizaÃ§Ã£o.

|                   **Aspecto**                   | **DescriÃ§Ã£o**                                                                                                                                                                                                                                                                                                                                                                               |
| :---------------------------------------------: | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
|       Treinamento Conjunto Imagem e VÃ­deo       | Para alavancar a vasta quantidade de dados de imagens, uma estratÃ©gia de **otimizaÃ§Ã£o alternada** intercala lotes de dados de imagem e vÃ­deo. Ã‰ usada uma **normalizaÃ§Ã£o especÃ­fica de domÃ­nio** para alinhar as distribuiÃ§Ãµes latentes e encorajar uma representaÃ§Ã£o isotrÃ³pica gaussiana. A perda de denoising para vÃ­deos Ã© escalonada para lidar com a convergÃªncia mais lenta.<br><br> |
|             Treinamento Progressivo             | O modelo Ã© treinado progressivamente, iniciando com **resoluÃ§Ãµes e duraÃ§Ãµes de vÃ­deo menores** (ex: 512p com 57 quadros) e avanÃ§ando para **resoluÃ§Ãµes e duraÃ§Ãµes maiores** (ex: 720p com 121 quadros). Uma fase de **"resfriamento" (cooling-down)** com dados de alta qualidade e uma taxa de aprendizado decrescente refina ainda mais o modelo.<br><br>                                 |
|            Treinamento Multi-Aspecto            | Os dados sÃ£o organizados em "buckets" com base em suas **proporÃ§Ãµes de aspecto** (ex: 1:1, 16:9) para acomodar a diversidade de conteÃºdo. **Preenchimento (padding) com reflexÃ£o** Ã© usado para pixels ausentes durante o processamento em lote.                                                                                                                                            |
|         Treinamento com PrecisÃ£o Mista          | Para eficiÃªncia, os pesos do modelo sÃ£o mantidos em **BF16 e FP32**. O BF16 Ã© usado para os passes de _forward_ e _backward_, e o FP32 para as atualizaÃ§Ãµes de parÃ¢metros, garantindo **estabilidade numÃ©rica**.                                                                                                                                                                            |
|            Condicionamento de Texto             | Utiliza o **T5-XXL** como codificador de texto. Modelos **Text2World** sÃ£o capazes de gerar vÃ­deo a partir de uma entrada textual.                                                                                                                                                                                                                                                          |
| Condicionamento de Imagem e VÃ­deo (Video2World) | Modelos **Video2World** estendem os modelos Text2World para aceitar quadros anteriores (imagem ou vÃ­deo) como condiÃ§Ã£o para gerar quadros futuros. RuÃ­do adicional Ã© introduzido nos quadros condicionais durante o treinamento para aumentar a robustez.                                                                                                                                   |

### OtimizaÃ§Ã£o de InferÃªncia: Tornando a GeraÃ§Ã£o RÃ¡pida

Embora os modelos de difusÃ£o sejam inerentemente mais lentos devido ao seu processo iterativo de denoising, otimizaÃ§Ãµes significativas sÃ£o aplicadas para acelerar a geraÃ§Ã£o.

#### TÃ©cnicas de OtimizaÃ§Ã£o de InferÃªncia

|                   **TÃ©cnica**                   | **DescriÃ§Ã£o**                                                                                                                                                                                                                                             |
| :---------------------------------------------: | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
|      FSDP (Fully Sharded Data Parallelism)      | Distribui os parÃ¢metros do modelo, gradientes e estados do otimizador por mÃºltiplos dispositivos (GPUs), resultando em significativa **economia de memÃ³ria** e permitindo o uso de modelos maiores.<br><br>                                               |
|            Context Parallelism (CP)             | Divide a computaÃ§Ã£o e as ativaÃ§Ãµes ao longo da dimensÃ£o da sequÃªncia, distribuindo-as entre GPUs. Esta tÃ©cnica Ã© crucial para lidar com **contextos longos de vÃ­deo**, onde a quantidade de dados a ser processada Ã© muito grande.<br><br>                |

### Prompt Upsampler: Para Entradas de Texto do UsuÃ¡rio

- Para preencher a lacuna entre as prompts de texto curtas e variadas fornecidas pelos usuÃ¡rios e as descriÃ§Ãµes de vÃ­deo detalhadas usadas no treinamento dos WFMs, um "Prompt Upsampler" Ã© desenvolvido.

- Ele transforma as prompts originais em versÃµes mais detalhadas e ricas que se alinham com a distribuiÃ§Ã£o das prompts de treinamento, melhorando a qualidade do vÃ­deo gerado. Para modelos Text2World, o Mistral-NeMo-12B-Instruct Ã© usado para isso; para Video2World, o Pixtral-12B Ã© utilizado.

### Decodificador de DifusÃ£o: Melhorando a Qualidade Visual do Autoregressivo

Embora este seja uma parte do modelo de difusÃ£o, ele tem um papel especial de pÃ³s-otimizaÃ§Ã£o para outros modelos:

- Para os modelos autoregressivos (que podem gerar vÃ­deos borrados devido Ã  tokenizaÃ§Ã£o agressiva), um decodificador de difusÃ£o mais poderoso Ã© usado como uma "pÃ³s-otimizaÃ§Ã£o".

- Este decodificador pega os tokens discretos (saÃ­da do modelo autoregressivo) e os "traduz" de volta para tokens contÃ­nuos de maior qualidade, que sÃ£o entÃ£o convertidos em vÃ­deos RGB de alta qualidade. Ã‰ como refinar um rascunho em uma obra de arte acabada.

### EquaÃ§Ãµes

**Perda do Denoising:** `â„’(ğ·ğœƒ, ğœ) = Ex0,n [ï¸âƒ¦âƒ¦ ğ·ğœƒ(x0 + n;ğœ)âˆ’ x0 âƒ¦âƒ¦2 2 ]ï¸`

- `x0 (lÃª-se "x zero")`: Representa o vÃ­deo original, limpo (a "tela perfeita")
- `n`: Representa o ruÃ­do gaussiano aleatÃ³rio que foi adicionado ao vÃ­deo x0
- `ğœ (sigma)`: Indica o nÃ­vel de ruÃ­do naquele momento. VÃ­deos com mais ruÃ­do terÃ£o um ğœ maior.
- `x0 + n`: Ã‰ o vÃ­deo com ruÃ­do (a "tela suja") que Ã© dado como entrada para o nosso modelo
- `ğ·ğœƒ`: Ã‰ a nossa rede neural "denoiser". O ğœƒ (theta) representa todos os parÃ¢metros (pesos) que a rede precisa aprender durante o treinamento
- `ğ·ğœƒ(x0 + n;ğœ)`: Ã‰ o que o modelo ğ·ğœƒ prevÃª que seja o vÃ­deo original limpo (x0), dado o vÃ­deo ruidoso (x0 + n) e o nÃ­vel de ruÃ­do (ğœ)
 - `ğ·ğœƒ(x0 + n;ğœ)âˆ’ x0`: Esta Ã© a diferenÃ§a entre o que o modelo previu e o vÃ­deo real e limpo (x0)
- ` ... âƒ¦âƒ¦2 2 ]ï¸`: Isso significa o quadrado da norma L2, que Ã© uma forma de medir a "distÃ¢ncia" ou o "erro" entre a previsÃ£o do modelo e a realidade. Basicamente, estamos pegando a diferenÃ§a, elevando ao quadrado (para que valores negativos e positivos contem igualmente) e somando tudo. Queremos que esse erro seja o menor possÃ­vel
- `E_x0,n [ ... ]`: Significa a esperanÃ§a (ou mÃ©dia) sobre diferentes vÃ­deos limpos (x0) e diferentes tipos de ruÃ­do (n)

**Perda total de Treinamento:** `â„’(ğ·ğœƒ) = Eğœ [ ğœ†(ğœ) â„’(ğ·ğœƒ, ğœ) + ğ‘¢(ğœ) ]`

- `Eğœ [ ... ]`: Significa a esperanÃ§a (mÃ©dia) sobre diferentes nÃ­veis de ruÃ­do (ğœ). O modelo Ã© treinado para lidar com todos os nÃ­veis de ruÃ­do, do quase limpo ao totalmente ruidoso.
- `ğœ†(ğœ) (lambda de sigma)`: Ã‰ uma funÃ§Ã£o de ponderaÃ§Ã£o. Ela ajusta a importÃ¢ncia de cada nÃ­vel de ruÃ­do (ğœ) na perda total, para que o modelo preste atenÃ§Ã£o a todos eles. Inicialmente, ela garante que todos os nÃ­veis de ruÃ­do contribuam igualmente para o aprendizado.
- `ğ‘¢(ğœ) (u de sigma)`: Ã‰ uma funÃ§Ã£o de incerteza contÃ­nua. O modelo tambÃ©m aprende essa funÃ§Ã£o. Se o modelo estÃ¡ "incerto" sobre como remover o ruÃ­do em um certo nÃ­vel ğœ, ele se penaliza, incentivando-o a reduzir essa incerteza. Isso ajuda a otimizaÃ§Ã£o em diferentes nÃ­veis de ruÃ­do, tratando-os como um problema de aprendizado multi-tarefa

**FunÃ§Ã£o de PonderaÃ§Ã£o:** `ğœ†(ğœ) = (ï¸€ ğœ2 + ğœ2data )ï¸€ / (ğœ Â· ğœdata)`

- `ğœdata`: Ã‰ o desvio padrÃ£o dos dados de treinamento. Essa equaÃ§Ã£o define como o ğœ†(ğœ) calcula o peso de cada nÃ­vel de ruÃ­do, inicialmente visando uma contribuiÃ§Ã£o igualitÃ¡ria

**DistribuiÃ§Ã£o do NÃ­vel de RuÃ­do:** `ln(ğœ) âˆ¼ ğ’© (ï¸€ ğ‘ƒmean, ğ‘ƒ 2std )ï¸€`

- Isso descreve como os nÃ­veis de ruÃ­do (ğœ) sÃ£o escolhidos durante o treinamento. O logaritmo natural (ln) de ğœ segue uma distribuiÃ§Ã£o normal (ğ’©), com uma mÃ©dia (ğ‘ƒmean) e um desvio padrÃ£o (ğ‘ƒstd) definidos. Isso garante que o modelo veja uma boa variedade de nÃ­veis de ruÃ­do

### Resultados e AplicaÃ§Ãµes

Os modelos de difusÃ£o Cosmos-1.0 (7B e 14B) sÃ£o capazes de gerar vÃ­deos com alta qualidade visual, dinÃ¢micas de movimento e alinhamento preciso com o texto. O modelo de 14B demonstra uma capacidade aprimorada de capturar detalhes visuais mais finos e padrÃµes de movimento mais intrincados.

Eles sÃ£o utilizados em diversas aplicaÃ§Ãµes de IA FÃ­sica, como:

- Controle de CÃ¢mera: Permitem gerar mundos virtuais navegÃ¡veis com base em uma imagem de referÃªncia e trajetÃ³rias de cÃ¢mera, mantendo a coerÃªncia 3D e temporal.

- ManipulaÃ§Ã£o RobÃ³tica: Podem ser ajustados para prever vÃ­deos de robÃ´s seguindo instruÃ§Ãµes de texto ou sequÃªncias de aÃ§Ãµes.

- ConduÃ§Ã£o AutÃ´noma: SÃ£o adaptados para criar modelos de mundo multi-visÃ£o para cenÃ¡rios de conduÃ§Ã£o, gerando vÃ­deos de seis cÃ¢meras simultaneamente e atÃ© seguindo trajetÃ³rias de veÃ­culos.

- Modelos de difusÃ£o baseados em Transformer sÃ£o frequentemente capazes de incorporar diversos sinais de controle.

- As avaliaÃ§Ãµes mostram que os WFMs baseados em difusÃ£o entregam melhor qualidade de geraÃ§Ã£o e maior consistÃªncia 3D em comparaÃ§Ã£o com as linhas de base e os modelos autoregressivos em certas condiÃ§Ãµes.

### LimitaÃ§Ãµes

Apesar dos avanÃ§os, os modelos de difusÃ£o para simulaÃ§Ã£o do mundo ainda enfrentam desafios comuns aos WFMs:

- Falta de PermanÃªncia de Objetos: Objetos podem desaparecer ou aparecer inesperadamente.

- ImprecisÃµes em DinÃ¢micas com Contato: InteraÃ§Ãµes fÃ­sicas complexas, como colisÃµes, ainda sÃ£o difÃ­ceis de modelar com precisÃ£o.

- InconsistÃªncia no Seguimento de InstruÃ§Ãµes: O modelo nem sempre segue as instruÃ§Ãµes de texto de forma totalmente precisa.

- AderÃªncia Ã s Leis da FÃ­sica: A gravidade, interaÃ§Ãµes de luz e dinÃ¢micas de fluidos ainda nÃ£o sÃ£o perfeitamente simuladas.