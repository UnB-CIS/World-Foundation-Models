{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6761a2c",
   "metadata": {},
   "source": [
    "# Stable Diffusion VAE - Variational Auto-Encoder\n",
    "\n",
    "This is a type of Auto-Encoder and Neural Network that trains using an unsupervised technique. They're widely used in image generation models, mainly on latent-diffusion and GAN based image generation models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddd5a39",
   "metadata": {},
   "source": [
    "## AutoEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df78067",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "To understand the VAE, we must first understand the Auto-Encoder.\n",
    "\n",
    "An Auto-Encoder is an unsupervised model primarily used for dimensionality reduction.\n",
    "\n",
    "We cannot feed 4K images to train the models, as they would be too big and unpractical to deal with. To solve this, we'll need a model that reduces the dimensionality of these images to a fixed small vector with high rich features from the originals.\n",
    "\n",
    "Auto-Encoders are designed to compress images into a latent space and back to the original image, so it lacks the ability to generate new and varied outputs.\n",
    "\n",
    "As expected, this is where the VAE comes from. Rather than memorizing the patterns, it generates variations of images. This means VAE doesn't directly generate the latent space, it first generates the probability distribution, only after producing the latent.\n",
    "\n",
    "By learning and predicting the probability distribution of the latent space, the VAE encodes each data point to a range defined by mean and variance, rather than a fixed vector. It generates \"pools\" of the data points in the latent space, rather than a single vector in that space. This allows variation within the pools, rather than fixing it to a single point.\n",
    "\n",
    "### Loss function\n",
    "\n",
    "To train the VAE, we use Reconstruction loss as MSE, combined with a scaled KL Divergence.\n",
    "\n",
    "#### Reconstruction Loss\n",
    "\n",
    "The Reconstruction loss is the MSE (Mean Squared Error) between original and reconstructed images, defined bellow:\n",
    "\n",
    "$$\\operatorname{Reconstruction\\ Loss} = \\frac{1}{N} \\ \\sum^N_{i=1}(x_i - \\hat{x_i})^2$$\n",
    "\n",
    "#### KL Divergence\n",
    "\n",
    "The KL Divergence (Kullback-Leibler divergence) is a measure of how different the learned distribution is from a standard normal distribution, defined as:\n",
    "\n",
    "$$\\operatorname{KL}(q(z|x) || p(z)) = -\\frac{1}{2} \\sum^J_{j=1} (1 + \\log \\sigma^2_j - \\mu^2_j - \\sigma^2_j)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\operatorname{KL}(q(z|x) || p(z))$ is the KL divergence from p of z to q of z given x. In other words, how different is the encoder's learned distribution from a standard normal distribution.\n",
    "- $q(z|x)$ (q of z given x) is the approximate/learned distribution.\n",
    "- $p(z)$ is the target/reference distribution.\n",
    "- $\\mu$ is the mean.\n",
    "- $\\sigma^2$ is the variance.\n",
    "- $J$ is the number of latent dimensions.\n",
    "\n",
    "This loss prevents the encoder from encoding each input to a very different, arbitrary location in latent space, and encourages the learned distribution to be close to $\\mathcal{N}(0,1)$. $1 + \\log \\sigma^2_j$ encourages variance to be close to 1, $\\mu^2_j$ penalizes mean far from 0, and $\\sigma^2_j$ penalizes far from 1.\n",
    "\n",
    "#### Final Loss\n",
    "\n",
    "The final loss is defined as the combination of the Reconstruction loss, with a weighted KL Divergence loss. It can be described as the sum of how well can we rebuild the images and how is the latent space structured. Were we to only use Reconstruction loss, the encoder would be able to map each image to arbitrary, distant points in latent space, but would lack structure and interpolation, and encourage overfitting. Whereas if we only used KL Divergence, the latent codes would all be $\\mathcal{N}(0, 1)$ (standard Normal/Gaussian distribution) but contain no information about the images, in other words a perfect distribution, but useless for reconstruction.\n",
    "\n",
    "The combination of both allows the encoder to follow a regular distribution pattern, and compress enough information to reconstruct the images.\n",
    "\n",
    "$$\\operatorname{loss} = \\operatorname{Reconstruction\\ Loss} + \\beta \\ \\operatorname{KL}(q(z|x) || p(z))$$\n",
    "\n",
    "Where $\\beta$ is a parameter to control how much to emphasize each loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66b0676",
   "metadata": {},
   "source": [
    "## Importing packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be6cf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import os\n",
    "import math\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb08eb9f",
   "metadata": {},
   "source": [
    "## Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885db7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(\n",
    "    source_dir: str,\n",
    "    train_dir: str,\n",
    "    test_dir: str,\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    ") -> None:\n",
    "    image_files = [\n",
    "        f\n",
    "        for f in os.listdir(source_dir)\n",
    "        if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "    ]\n",
    "\n",
    "    train_files, test_files = train_test_split(\n",
    "        image_files, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    for file in train_files:\n",
    "        shutil.copy(os.path.join(source_dir, file), os.path.join(train_dir, file))\n",
    "\n",
    "    for file in test_files:\n",
    "        shutil.copy(os.path.join(source_dir, file), os.path.join(test_dir, file))\n",
    "\n",
    "    print(\n",
    "        f\"Dataset split complete. {len(train_files)} training images, {len(test_files)} test images.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1be60c",
   "metadata": {},
   "source": [
    "## Classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e9e6d0",
   "metadata": {},
   "source": [
    "### Self-Attention Block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01fb2dc",
   "metadata": {},
   "source": [
    "This block implements the self-attention algorithm from [Attention is all you need](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V)= \\operatorname{softmax}(\\frac{Q\\cdot K^T}{\\sqrt{d_k}})\\cdot V\n",
    "$$\n",
    "\n",
    "For this implementation there are a few quirks, so they'll be described bellow:\n",
    "\n",
    "- `in_proj` and `out_proj`: These layers are the first and final transformations to the data. The input projection layer, projects the input embeddings into the query, key and value spaces for attention computation. Whereas the output projection layer, projects its inputs back to the original embedding dimension, after the computation of the attention weighted values.\n",
    "\n",
    "- _Interim shape_: The interim shape is a reorganization format that explicitly separates the embedding dimension into multiple attention heads. This is essential for computing attention, where each head learns different representations subspaces independently. The interim shape in the code is `[batch_size, seq_len, self.n_heads, self.d_heads].\n",
    "\n",
    "- `.chunk` splits a tensor into equal-sized pieces along a specified dimension. In the context of the code, it takes the output of the input projection and splits into three equal tensors along the last dimension. `[batch_size, seq_len, 3 * embed_dim] -> 3 * [batch_size, seq_len, embed_dim]`\n",
    "\n",
    "- `q, v, k` are transposed in order to make parallel computation possible across all attention heads. This is done for conformity with PyTorch's broadcasting method for matmul. Matmul follows these rules:\n",
    "\n",
    "  1. The multiplication only happens in the last two dimensions.\n",
    "  2. All leading dimensions are treated as batch dimensions.\n",
    "  3. The operation broadcasts across these batch dimension.\n",
    "\n",
    "  - The actual computation process is as follows:\n",
    "  - ```text\n",
    "    q @ k.transpose(-1, -2)\n",
    "\n",
    "    For each combination of (batch_idx, head_idx):\n",
    "    Take q[batch_idx, head_idx, :, :] -> shape (seq_len, d_head)\n",
    "    Take k[batch_idx, head_idx, :, :] -> shape (seq_len, d_head)\n",
    "    Compute: q[batch_idx, head_idx] @ k[batch_idx, head_idx].T\n",
    "    Result: (seq_len, seq_len) attention scores\n",
    "    ```\n",
    "\n",
    "#### Forward pass step-by-step\n",
    "\n",
    "1. Input projection and chunking\n",
    "   - Projects input into query, key, and value spaces.\n",
    "2. Reshape to interim shape\n",
    "   - Splits the embedding dimension in to `n_heads` separate subspaces, enabling multi-head attention where each head learns different patterns.\n",
    "3. Transpose for parallel computation\n",
    "   - Moves heads to the second dimension so all heads can be computed in parallel.\n",
    "4. Compute attention scores\n",
    "   - Computes Similarity scores between every pair of positions in the sequence.\n",
    "5. Apply causal mask\n",
    "   - For autoregressive models, prevents positions from attending to future positions by setting their scores to negative infinity, that outputs to 0 after softmax.\n",
    "6. Scale and normalize\n",
    "   - Scales the dot products by $\\sqrt{{d\\_head}}$ to prevent them from growing too large.\n",
    "7. Compute weighted values\n",
    "   - Aggregates values from all positions, weighted by attention probabilities.\n",
    "8. Transpose back\n",
    "   - Reverses the earlier transpose to prepare for concatenating all heads back together.\n",
    "9. Reshape to original format\n",
    "   - Concatenates all attention heads back into a single embedding dimension, merging the learned learned representations from all heads.\n",
    "10. Output Projection\n",
    "    - Final linear transformation that allows the model to learn how to combine information from different heads optimally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe40ecff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_heads,\n",
    "        embed_dim,\n",
    "        in_proj_bias=True,\n",
    "        out_proj_bias=True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.in_proj = nn.Linear(embed_dim, 3 * embed_dim, bias=in_proj_bias)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=out_proj_bias)\n",
    "\n",
    "        self.d_heads = embed_dim // n_heads\n",
    "\n",
    "    def forward(self, x: torch.Tensor, causal_mask=False) -> torch.Tensor:\n",
    "        batch_size, seq_len, channels = x.shape\n",
    "\n",
    "        interim_shape = (batch_size, seq_len, self.n_heads, self.d_heads)\n",
    "\n",
    "        # Input projection and chunking\n",
    "        q, k, v = self.in_proj(x).chunk(3, dim=-1)\n",
    "\n",
    "        # Reshaping tensors to interim shape\n",
    "        q = q.view(interim_shape)\n",
    "        k = k.view(interim_shape)\n",
    "        v = v.view(interim_shape)\n",
    "\n",
    "        # Transposing\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        # Computing first part of attention\n",
    "        weight = q @ k.transpose(-1, -2)  # Matmul\n",
    "\n",
    "        # Applying mask\n",
    "        if causal_mask:\n",
    "            # Mask where the upper triangle (above the principal diagonal) is 1\n",
    "            mask = torch.ones_like(weight, dtype=torch.bool).triu()\n",
    "\n",
    "            # Fill mask values with -inf\n",
    "            weight.masked_fill(mask, -torch.inf)\n",
    "\n",
    "        # Dividing by square root of d_heads as described in paper\n",
    "        weight /= math.sqrt(self.d_heads)\n",
    "\n",
    "        weight = F.softmax(weight, dim=-1)\n",
    "\n",
    "        # Final computing of attention\n",
    "        output = weight @ v\n",
    "\n",
    "        # Returning to original shape\n",
    "        output = output.transpose(1, 2)\n",
    "\n",
    "        # Changing the shape to the shape of out_proj\n",
    "        output = output.reshape((batch_size, seq_len, channels))\n",
    "\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4cdd9e",
   "metadata": {},
   "source": [
    "### Attention Block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eb4ab1",
   "metadata": {},
   "source": [
    "This block is still mostly from the original transformer architecture's paper, but it has a more focused approach to vision-based attention. The first evidence of this, is the fact the block uses `GroupNorm` instead of `LayerNorm`.\n",
    "\n",
    "The two layers from PyTorch implement the same operation, with the key difference between the two being that `GroupNorm` applies Normalization into groups of channels (32 in the code bellow), whereas `LayerNorm` applies it into the entire channels dimension. Both layers implement the following formula:\n",
    "\n",
    "$$y = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $x$ is the input.\n",
    "- $\\mu$ is the input's mean.\n",
    "- $\\sigma$ is the standard deviation.\n",
    "- $\\gamma$ is a learnable scale parameter.\n",
    "- $\\beta$ is a leanable shift parameter.\n",
    "- $\\epsilon$ is a small constant for numerical stability.\n",
    "\n",
    "And the difference is that each of these variables is measured from within the groups.\n",
    "\n",
    "#### Forward pass step-by-step\n",
    "\n",
    "1. Save the residual connection\n",
    "   - Saves the original data to add as residual connection post self-attention layer.\n",
    "2. Group Normalization\n",
    "   - Group normalization for activation stability, so that attention scores don't explode or vanish.\n",
    "3. Reshape to sequence format\n",
    "   - Flattens the inputs so the images are sequential.\n",
    "4. Transpose to Sequence format\n",
    "   - Makes channels last dimension to conform with self-attention layer's expected format.\n",
    "5. Apply Self-Attention\n",
    "   - Allows spatial location to \"see\" the entire image, capturing global structure.\n",
    "6. Transpose back to spatial format\n",
    "   - Reverse the transpose.\n",
    "7. Reshape back to 2D spatial\n",
    "   - Reverse the image flattening.\n",
    "8. Sum with residual connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743af9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, channels) -> None:\n",
    "        super().__init__()\n",
    "        self.group_norm = nn.GroupNorm(32, channels)\n",
    "        self.attention = SelfAttention(1, channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        residual = x.clone()\n",
    "\n",
    "        x = self.group_norm(x)\n",
    "\n",
    "        n_samples, channels, height, width = x.shape\n",
    "\n",
    "        x = x.view((n_samples, channels, height * width))\n",
    "\n",
    "        x = x.transpose(-1, -2)\n",
    "\n",
    "        x = self.attention(x)\n",
    "\n",
    "        x = x.transpose(-1, -2)\n",
    "\n",
    "        print(x.shape)\n",
    "\n",
    "        x = x.view((n_samples, channels, height, width))\n",
    "\n",
    "        x += residual\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38118e5",
   "metadata": {},
   "source": [
    "### Residual Block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd99b9a",
   "metadata": {},
   "source": [
    "Residual Blocks on the other hand, don't come from the Attention is all you Need paper, instead it originates form the ResNet architecture, first proposed in [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385). This architecture is composed of two sequential weight layers, whose outputs are combined with the inputs to its layers.\n",
    "\n",
    "Although the idea is simple enough, the implementation has some steps worth going over:\n",
    "\n",
    "- `in_channels == out_channels`: This if block is supposed to handle if the number of channels is supposed to change. We wouldn't be able to add the original input to the output of the second weighted layer if the number of channels has increased, to handle this we pass the inputs through a 1D convolution that only increases the number of channels, preserving the spatial dimensions of the data. It ensures this by passing the input through a `torch.nn.Identity` layer, that doesn't perform any operations ($\\operatorname{Identity}(x) = x$).\n",
    "\n",
    "#### Forward pass step-by-step\n",
    "\n",
    "1. Save the residual path\n",
    "2. Group Normalization 1\n",
    "3. Activation function\n",
    "4. First convolution\n",
    "5. Group Normalization 2\n",
    "6. Second Convolution\n",
    "7. Residual Connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38301bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels) -> None:\n",
    "        super().__init__()\n",
    "        self.group_norm_1 = nn.GroupNorm(32, in_channels)\n",
    "        self.conv_1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        self.group_norm_2 = nn.GroupNorm(32, out_channels)\n",
    "        self.conv_2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        if in_channels == out_channels:\n",
    "            self.residual_layer = nn.Identity()\n",
    "        else:\n",
    "            self.residual_layer = nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size=1, padding=0\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        residue = x.clone()\n",
    "\n",
    "        x = self.group_norm_1(x)\n",
    "        x = F.selu(x)\n",
    "        x = self.conv_1(x)\n",
    "        x = self.group_norm_2(x)\n",
    "        x = self.conv_2(x)\n",
    "\n",
    "        return x + self.residual_layer(residue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cafb581",
   "metadata": {},
   "source": [
    "### Encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18173ba2",
   "metadata": {},
   "source": [
    "This is one of the two main components of the VAE, responsible for compressing high-dimensional input data into a low-dimensional latent representation. Rather than mapping the data to a single point, it maps them to a probability distribution in latent space.\n",
    "\n",
    "The entire idea of this block is that the sequential layer of blocks will output a tensor with 8 channels, that are then divided, the first 4 will represent the mean, and the last 4 the log variance of the latent space. Once the channels have been split in half, we fit the data through a sampling equation:\n",
    "\n",
    "$$z = \\mu + \\epsilon \\cdot \\sigma$$\n",
    "\n",
    "Where $\\mu$ is the mean, $\\epsilon$ random noise, and $\\sigma$ is the standard deviation. At first neither $\\mu$ or $log \\ \\sigma^2$ actually represent the true mean and variance, but during training the model will learn these values for a given input.\n",
    "\n",
    "As most of the code blocks so far, this one has its peculiarities:\n",
    "\n",
    "- Inherits from `torch.nn.Sequential` instead of `torch.nn.Module`: this is basically the same as using `nn.Sequential` for chaining layers, it automatically registers parameters, and supports layer indexing (biggest reason for using this architecture).\n",
    "\n",
    "#### Forward pass step-by-step\n",
    "\n",
    "1. Loop through sequential layers\n",
    "   - The for loop iterates through all layers in the class. Inside the loop, there is special handling for 2D convolutions, where we add a pixel on the right and bottom of the input, in order to compensate for the asymmetry of stride-2 convolution with `padding=0`.\n",
    "2. Layer by layer transformation\n",
    "   - Just applies the layers to the data.\n",
    "3. Splitting channels\n",
    "   - The outputs of the convolutional layer are chunked into two tensors, each with 4 channels. The first one is the mean, and the second one the log variance.\n",
    "4. Clamp Log-Variance\n",
    "   - Using `torch.clamp`, we clamp the values to a range $[-30, 20]$. This is done to prevent $log(\\sigma^2)$ from becoming too extreme, ensuring numerical stability.\n",
    "5. Sample from distribution\n",
    "   - Reparametrization trick for sampling from the learned distribution, allowing backpropagation through the sampling.\n",
    "6. Scale the latent space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b578c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
    "            ResidualBlock(128, 128),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=0),\n",
    "            ResidualBlock(128, 256),\n",
    "            ResidualBlock(256, 256),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=0),\n",
    "            ResidualBlock(256, 512),\n",
    "            ResidualBlock(512, 512),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=0),\n",
    "            ResidualBlock(512, 512),\n",
    "            ResidualBlock(512, 512),\n",
    "            ResidualBlock(512, 512),\n",
    "            AttentionBlock(512),\n",
    "            ResidualBlock(512, 512),\n",
    "            nn.GroupNorm(32, 512),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(512, 8, kernel_size=3, padding=1),\n",
    "            nn.Conv2d(8, 8, kernel_size=1, padding=0),\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        x = input\n",
    "\n",
    "        for module in self:\n",
    "            if isinstance(module, nn.Conv2d) and module.stride == (2, 2):\n",
    "                x = F.pad(x, (0, 1, 0, 1))\n",
    "            x = module(x)\n",
    "\n",
    "        mean, log_variance = torch.chunk(x, 2, dim=1)\n",
    "\n",
    "        log_variance = torch.clamp(log_variance, -30, 20)\n",
    "\n",
    "        std = torch.std(0.5 * log_variance)\n",
    "        eps = torch.rand_like(std)\n",
    "        x = mean + eps * std\n",
    "\n",
    "        x *= 0.18215\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8e337e",
   "metadata": {},
   "source": [
    "### Decoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ca4d87",
   "metadata": {},
   "source": [
    "The Decoder is the mirror of the Encoder. While the Encoder compresses, the decoder reconstructs by reversing the Encoder's operations.\n",
    "\n",
    "#### Forward Pass step-by-step\n",
    "\n",
    "1. Receive latent code\n",
    "   - The compressed representation to expand back to an image.\n",
    "2. Undo scaling\n",
    "   - Reverses the unit variance scaling from the Encoder.\n",
    "3. Progressive upsampling\n",
    "4. Spatial Upsampling\n",
    "5. Channel reduction\n",
    "6. Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f2ae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(4, 512, kernel_size=3, padding=1),\n",
    "            ResidualBlock(512, 512),\n",
    "            AttentionBlock(512),\n",
    "            ResidualBlock(512, 512),\n",
    "            ResidualBlock(512, 512),\n",
    "            ResidualBlock(512, 512),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            ResidualBlock(512, 512),\n",
    "            ResidualBlock(512, 512),\n",
    "            ResidualBlock(512, 512),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            ResidualBlock(512, 256),\n",
    "            ResidualBlock(256, 256),\n",
    "            ResidualBlock(256, 256),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            ResidualBlock(256, 128),\n",
    "            ResidualBlock(128, 128),\n",
    "            ResidualBlock(128, 128),\n",
    "            nn.GroupNorm(32, 128),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(128, 3, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        x = input\n",
    "\n",
    "        x /= 0.18215\n",
    "\n",
    "        for module in self:\n",
    "            x = module(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb76b693",
   "metadata": {},
   "source": [
    "### VAE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8901a27c",
   "metadata": {},
   "source": [
    "Finally, the VAE architecture, composed of the encoder and the decoder sequentially.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3db2707",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "\n",
    "        return decoded, encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3816ddbf",
   "metadata": {},
   "source": [
    "## Downloading and preprocessing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4708def",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown 1KXRTB_q4uub_XOHecpsQjE4Kmv76sZbV\n",
    "\n",
    "!unzip -q all-dogs.zip\n",
    "\n",
    "!mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84a81de",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = \"./all-dogs\"\n",
    "train_dir = \"./data/train/dogs\"\n",
    "test_dir = \"./data/test/dogs\"\n",
    "\n",
    "split_dataset(source_dir, train_dir, test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb0856a",
   "metadata": {},
   "source": [
    "## Training model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15968c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall configurations\n",
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cuda\")\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 100\n",
    "learning_rate = 1e-4\n",
    "beta = 0.00025\n",
    "batch_size = 4\n",
    "accumulation_steps = 1\n",
    "effective_batch_size = batch_size * accumulation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d134a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((56, 56)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    ")\n",
    "dataset = torchvision.datasets.ImageFolder(root=\"./data/train\", transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632d9ce4",
   "metadata": {},
   "source": [
    "Dividing the loss by `accumulation_steps` simulates bigger batches, accumulating gradients over only after $n$ batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd20834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for i, (images, _) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "\n",
    "        reconstructed, encoded = model(images)\n",
    "        recon_loss = nn.MSELoss()(reconstructed, images)\n",
    "\n",
    "        mean, log_variance = torch.chunk(encoded, 2, dim=1)\n",
    "\n",
    "        kl_div = -0.5 * torch.sum(1 + log_variance - mean.pow(2) - log_variance.exp())\n",
    "        loss = recon_loss + beta * kl_div\n",
    "\n",
    "        loss = loss / accumulation_steps\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item() * accumulation_steps\n",
    "\n",
    "        print(\n",
    "            f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], '\n",
    "            f'Loss: {loss.item()*accumulation_steps:.4f}, Recon Loss: {recon_loss.item():.4f}, KL Div: {kl_div.item():.4f}'\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            sample_image = images[0].unsqueeze(0)\n",
    "            sample_reconstructed = model(sample_image)[0]\n",
    "\n",
    "            sample_image = (sample_image * 0.5) + 0.5\n",
    "\n",
    "            torchvision.utils.save_image(sample_reconstructed, \"reconstructed.png\")\n",
    "\n",
    "    train_losses.append(train_loss / len(dataloader))\n",
    "\n",
    "    torch.save(model.state_dict(), f=f\"models/vae_model_epoch_{epoch + 1}.pth\")\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7164daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting loss curve\n",
    "plt.figure(figsize=(10, 15))\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"VAE Loss over Time\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
