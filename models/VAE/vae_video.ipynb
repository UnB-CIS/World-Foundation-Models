{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "546e8853",
      "metadata": {
        "id": "546e8853"
      },
      "source": [
        "# VAE for Video\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f40ebdc",
      "metadata": {
        "id": "2f40ebdc"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39172238",
      "metadata": {
        "id": "39172238"
      },
      "source": [
        "### Installing packages and data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "782d8b27",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "782d8b27",
        "outputId": "ce8880a6-5a2e-4396-8f56-8992e4f123b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6be983d5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6be983d5",
        "outputId": "64d2540b-c80d-4855-9612-8844deef1d4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 87.8M  100 87.8M    0     0  48.1M      0  0:00:01  0:00:01 --:--:-- 48.1M\n",
            "Archive:  dataset.zip\n",
            "   creating: dataset/\n",
            "  inflating: dataset/scenarios_docs.md  \n",
            "   creating: dataset/scenario2/\n",
            "  inflating: dataset/sim.mp4         \n",
            "   creating: dataset/scenario1/\n",
            "  inflating: dataset/pymunk_test.py  \n",
            "  inflating: dataset/scenario2/scenario2.py  \n",
            "  inflating: dataset/scenario1/batch_runner.py  \n",
            "  inflating: dataset/scenario1/generate_random_inputs.py  \n",
            "   creating: dataset/scenario1/__pycache__/\n",
            "   creating: dataset/scenario1/videos/\n",
            "  inflating: dataset/scenario1/scenario1.py  \n",
            "   creating: dataset/scenario1/inputs/\n",
            "  inflating: dataset/scenario1/__pycache__/scenario1.cpython-312.pyc  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_116_auto_05-10-2025_16-20-16.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_157_auto_05-10-2025_16-23-54.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_70_auto_05-10-2025_16-19-38.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_138_auto_05-10-2025_16-18-40.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_76_auto_05-10-2025_16-20-30.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_192_auto_05-10-2025_16-20-54.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_183_auto_05-10-2025_16-20-07.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_111_auto_05-10-2025_16-19-05.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_171_auto_05-10-2025_16-20-32.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_11_auto_05-10-2025_16-22-32.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_196_auto_05-10-2025_16-22-17.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_105_auto_05-10-2025_16-21-27.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_168_auto_05-10-2025_16-21-51.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_09_auto_05-10-2025_16-20-24.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_63_auto_05-10-2025_16-21-44.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_160_auto_05-10-2025_16-23-26.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_154_auto_05-10-2025_16-20-03.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_151_auto_05-10-2025_16-21-22.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_83_auto_05-10-2025_16-23-28.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_109_auto_05-10-2025_16-19-16.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_07_auto_05-10-2025_16-22-34.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_123_auto_05-10-2025_16-22-43.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_87_auto_05-10-2025_16-22-06.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_188_auto_05-10-2025_16-19-20.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_39_auto_05-10-2025_16-20-02.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_96_auto_05-10-2025_16-20-44.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_122_auto_05-10-2025_16-23-19.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_17_auto_05-10-2025_16-24-24.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_98_auto_05-10-2025_16-23-05.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_115_auto_05-10-2025_16-21-55.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_56_auto_05-10-2025_16-21-07.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_198_auto_05-10-2025_16-21-41.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_195_auto_05-10-2025_16-20-04.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_158_auto_05-10-2025_16-19-48.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_107_auto_05-10-2025_16-21-02.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_149_auto_05-10-2025_16-24-07.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_04_auto_05-10-2025_16-22-20.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_120_auto_05-10-2025_16-19-02.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_118_auto_05-10-2025_16-20-39.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_112_auto_05-10-2025_16-19-06.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_52_auto_05-10-2025_16-22-10.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_191_auto_05-10-2025_16-20-42.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_136_auto_05-10-2025_16-22-44.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_194_auto_05-10-2025_16-22-30.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_45_auto_05-10-2025_16-19-58.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_26_auto_05-10-2025_16-22-53.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_89_auto_05-10-2025_16-19-40.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_32_auto_05-10-2025_16-22-49.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_75_auto_05-10-2025_16-22-08.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_146_auto_05-10-2025_16-22-57.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_141_auto_05-10-2025_16-22-29.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_159_auto_05-10-2025_16-20-01.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_95_auto_05-10-2025_16-22-42.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_88_auto_05-10-2025_16-18-40.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_86_auto_05-10-2025_16-18-40.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_71_auto_05-10-2025_16-21-45.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_181_auto_05-10-2025_16-20-47.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_84_auto_05-10-2025_16-24-17.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_66_auto_05-10-2025_16-20-26.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_74_auto_05-10-2025_16-21-18.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_186_auto_05-10-2025_16-22-15.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_47_auto_05-10-2025_16-21-18.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_44_auto_05-10-2025_16-23-30.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_139_auto_05-10-2025_16-20-28.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_42_auto_05-10-2025_16-22-00.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_77_auto_05-10-2025_16-21-22.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_78_auto_05-10-2025_16-21-10.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_38_auto_05-10-2025_16-24-22.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_92_auto_05-10-2025_16-18-59.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_01_auto_05-10-2025_16-22-32.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_114_auto_05-10-2025_16-23-38.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_128_auto_05-10-2025_16-23-15.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_94_auto_05-10-2025_16-23-41.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_15_auto_05-10-2025_16-23-41.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_53_auto_05-10-2025_16-19-39.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_176_auto_05-10-2025_16-22-29.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_102_auto_05-10-2025_16-21-11.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_55_auto_05-10-2025_16-19-49.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_133_auto_05-10-2025_16-18-40.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_152_auto_05-10-2025_16-22-10.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_172_auto_05-10-2025_16-20-47.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_162_auto_05-10-2025_16-20-32.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_97_auto_05-10-2025_16-20-11.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_130_auto_05-10-2025_16-22-54.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_16_auto_05-10-2025_16-21-45.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_10_auto_05-10-2025_16-19-25.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_61_auto_05-10-2025_16-24-31.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_85_auto_05-10-2025_16-23-02.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_189_auto_05-10-2025_16-19-48.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_165_auto_05-10-2025_16-21-25.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_113_auto_05-10-2025_16-23-56.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_127_auto_05-10-2025_16-23-36.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_104_auto_05-10-2025_16-22-09.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_62_auto_05-10-2025_16-24-09.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_91_auto_05-10-2025_16-23-27.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_14_auto_05-10-2025_16-20-19.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_36_auto_05-10-2025_16-23-05.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_101_auto_05-10-2025_16-22-56.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_129_auto_05-10-2025_16-23-34.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_03_auto_05-10-2025_16-23-43.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_28_auto_05-10-2025_16-20-21.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_150_auto_05-10-2025_16-21-49.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_164_auto_05-10-2025_16-19-05.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_180_auto_05-10-2025_16-22-48.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_134_auto_05-10-2025_16-21-54.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_05_auto_05-10-2025_16-24-13.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_13_auto_05-10-2025_16-23-23.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_20_auto_05-10-2025_16-18-40.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_54_auto_05-10-2025_16-20-11.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_132_auto_05-10-2025_16-24-00.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_166_auto_05-10-2025_16-19-33.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_69_auto_05-10-2025_16-21-50.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_12_auto_05-10-2025_16-21-28.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_184_auto_05-10-2025_16-22-33.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_177_auto_05-10-2025_16-21-55.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_49_auto_05-10-2025_16-18-40.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_153_auto_05-10-2025_16-20-14.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_185_auto_05-10-2025_16-20-35.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_197_auto_05-10-2025_16-21-37.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_156_auto_05-10-2025_16-19-25.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_108_auto_05-10-2025_16-22-31.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_161_auto_05-10-2025_16-20-11.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_34_auto_05-10-2025_16-19-02.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_179_auto_05-10-2025_16-22-05.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_64_auto_05-10-2025_16-23-57.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_170_auto_05-10-2025_16-21-14.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_155_auto_05-10-2025_16-19-37.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_02_auto_05-10-2025_16-19-43.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_18_auto_05-10-2025_16-22-30.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_182_auto_05-10-2025_16-23-30.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_175_auto_05-10-2025_16-22-19.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_119_auto_05-10-2025_16-20-58.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_100_auto_05-10-2025_16-18-57.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_48_auto_05-10-2025_16-18-58.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_144_auto_05-10-2025_16-19-49.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_22_auto_05-10-2025_16-18-40.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_57_auto_05-10-2025_16-20-23.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_65_auto_05-10-2025_16-20-38.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_200_auto_05-10-2025_16-19-18.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_199_auto_05-10-2025_16-22-48.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_06_auto_05-10-2025_16-21-04.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_173_auto_05-10-2025_16-21-29.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_126_auto_05-10-2025_16-24-10.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_125_auto_05-10-2025_16-19-35.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_19_auto_05-10-2025_16-22-06.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_140_auto_05-10-2025_16-21-37.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_169_auto_05-10-2025_16-23-22.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_33_auto_05-10-2025_16-19-52.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_103_auto_05-10-2025_16-21-13.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_21_auto_05-10-2025_16-19-04.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_08_auto_05-10-2025_16-18-54.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_72_auto_05-10-2025_16-20-32.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_29_auto_05-10-2025_16-22-37.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_124_auto_05-10-2025_16-24-19.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_23_auto_05-10-2025_16-21-21.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_58_auto_05-10-2025_16-22-16.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_79_auto_05-10-2025_16-18-40.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_60_auto_05-10-2025_16-18-40.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_90_auto_05-10-2025_16-19-20.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_131_auto_05-10-2025_16-21-00.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_51_auto_05-10-2025_16-24-35.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_30_auto_05-10-2025_16-19-40.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_40_auto_05-10-2025_16-20-59.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_59_auto_05-10-2025_16-20-46.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_178_auto_05-10-2025_16-20-50.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_73_auto_05-10-2025_16-21-30.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_110_auto_05-10-2025_16-19-16.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_187_auto_05-10-2025_16-23-19.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_24_auto_05-10-2025_16-18-40.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_163_auto_05-10-2025_16-23-14.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_81_auto_05-10-2025_16-23-00.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_31_auto_05-10-2025_16-22-57.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_67_auto_05-10-2025_16-21-29.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_25_auto_05-10-2025_16-19-18.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_137_auto_05-10-2025_16-20-41.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_68_auto_05-10-2025_16-19-36.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_147_auto_05-10-2025_16-23-16.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_46_auto_05-10-2025_16-20-03.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_148_auto_05-10-2025_16-23-08.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_106_auto_05-10-2025_16-18-40.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_143_auto_05-10-2025_16-19-40.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_190_auto_05-10-2025_16-18-54.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_167_auto_05-10-2025_16-19-41.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_50_auto_05-10-2025_16-20-16.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_80_auto_05-10-2025_16-24-00.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_121_auto_05-10-2025_16-19-04.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_174_auto_05-10-2025_16-20-49.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_35_auto_05-10-2025_16-21-44.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_99_auto_05-10-2025_16-19-16.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_117_auto_05-10-2025_16-22-13.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_145_auto_05-10-2025_16-19-25.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_193_auto_05-10-2025_16-21-14.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_43_auto_05-10-2025_16-19-21.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_135_auto_05-10-2025_16-20-59.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_37_auto_05-10-2025_16-20-24.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_82_auto_05-10-2025_16-18-40.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_93_auto_05-10-2025_16-20-02.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_27_auto_05-10-2025_16-23-15.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161830_41_auto_05-10-2025_16-19-17.mp4  \n",
            "  inflating: dataset/scenario1/videos/random_input_20251005161831_142_auto_05-10-2025_16-22-19.mp4  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_156.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_101.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_117.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_140.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_10.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_47.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_183.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_85.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_195.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_51.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_93.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_06.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_26.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_30.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_137.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_89.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_66.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_160.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_70.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_176.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_199.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_121.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_120.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_177.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_198.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_71.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_161.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_88.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_67.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_136.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_31.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_27.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_07.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_92.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_194.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_50.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_84.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_46.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_182.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_11.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_141.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_116.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_100.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_157.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_20.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_36.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_61.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_131.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_166.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_189.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_99.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_76.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_170.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_127.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_150.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_107.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_111.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_146.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_16.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_185.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_41.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_83.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_57.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_193.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_95.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_01.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_94.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_56.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_192.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_82.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_184.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_40.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_17.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_147.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_110.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_106.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_151.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_126.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_171.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_98.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_77.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_167.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_188.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_130.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_60.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_37.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_21.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_125.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_59.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_172.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_74.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_164.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_18.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_133.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_148.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_63.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_34.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_109.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_22.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_200.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_02.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_129.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_78.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_97.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_191.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_55.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_81.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_43.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_187.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_168.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_14.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_144.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_113.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_38.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_105.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_152.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_153.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_104.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_112.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_39.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_145.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_15.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_42.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_186.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_169.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_80.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_190.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_54.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_79.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_96.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_03.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_128.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_108.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_23.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_35.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_149.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_62.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_19.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_132.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_165.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_75.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_58.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_173.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_124.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_04.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_91.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_53.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_197.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_178.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_68.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_87.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_181.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_45.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_12.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_139.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_142.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_115.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_103.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_28.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_154.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_08.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_123.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_174.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_72.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_49.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_162.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_135.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_119.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_32.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_24.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_158.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_159.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_25.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_118.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_33.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_64.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_134.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_65.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_48.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_163.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_73.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_175.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_09.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_122.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_155.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_102.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_29.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_114.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_143.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_13.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_138.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_180.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_44.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_69.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_86.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_52.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_196.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_179.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161831_90.json  \n",
            "  inflating: dataset/scenario1/inputs/random_input_20251005161830_05.json  \n"
          ]
        }
      ],
      "source": [
        "!curl \"https://drive.usercontent.google.com/download?id={1QmhwyHHs1w08jUEk6ygp3ULR_UJrKAdG}&confirm=xxx\" -o dataset.zip\n",
        "!unzip dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "417c3235",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "417c3235",
        "outputId": "0fe22437-4055-4a4e-c56b-e4b68e971500"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['.config', 'dataset.zip', 'dataset', 'sample_data']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.listdir()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2e2ee68",
      "metadata": {
        "id": "f2e2ee68"
      },
      "source": [
        "### Importing packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7c3603eb",
      "metadata": {
        "id": "7c3603eb"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "import os\n",
        "import math\n",
        "import shutil\n",
        "from PIL import Image\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2eda2f85",
      "metadata": {
        "id": "2eda2f85"
      },
      "source": [
        "### Functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac6cfe57",
      "metadata": {
        "id": "ac6cfe57"
      },
      "source": [
        "#### Video preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "69b3aa30",
      "metadata": {
        "id": "69b3aa30"
      },
      "outputs": [],
      "source": [
        "def extract_frames(video_path, frames_dir, frame_step=5):\n",
        "    os.makedirs(frames_dir, exist_ok=True)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_count = 0\n",
        "    saved_count = 0\n",
        "\n",
        "    while True:\n",
        "        success, frame = cap.read()\n",
        "        if not success:\n",
        "            break\n",
        "\n",
        "        if frame_count % frame_step == 0:\n",
        "            frame_path = os.path.join(frames_dir, f\"frame_{saved_count:04d}.jpg\")\n",
        "            cv2.imwrite(frame_path, frame)\n",
        "            saved_count += 1\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    print(\n",
        "        f\"✅ Extraídos {saved_count} frames (de {frame_count} totais) para {frames_dir}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2eb3ab1e",
      "metadata": {
        "id": "2eb3ab1e"
      },
      "outputs": [],
      "source": [
        "def process_videos(videos_folder, frames_root=\"./frames\", frame_step=5):\n",
        "    for video_file in os.listdir(videos_folder):\n",
        "        if video_file.lower().endswith((\".mp4\", \".avi\", \".mov\")):\n",
        "            video_path = os.path.join(videos_folder, video_file)\n",
        "            # Cria subpasta para cada vídeo\n",
        "            video_name = os.path.splitext(video_file)[0]\n",
        "            frames_dir = os.path.join(frames_root, video_name)\n",
        "            # Extrai frames se a pasta estiver vazia\n",
        "            if not os.path.exists(frames_dir) or len(os.listdir(frames_dir)) == 0:\n",
        "                extract_frames(video_path, frames_dir, frame_step=frame_step)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc0644f0",
      "metadata": {},
      "source": [
        "#### Loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1eb950ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "def aggressive_beta(epoch, batch_idx, num_batches, warmup_epochs=8):\n",
        "    \"\"\"\n",
        "    MUCH faster beta ramp for difficult data\n",
        "    Reach β=1.0 in 8 epochs (was 15)\n",
        "    \"\"\"\n",
        "    total_steps = warmup_epochs * num_batches\n",
        "    current_step = epoch * num_batches + batch_idx\n",
        "\n",
        "    if current_step >= total_steps:\n",
        "        return 1.0\n",
        "\n",
        "    # Cubic ramp for even faster growth\n",
        "    progress = current_step / total_steps\n",
        "    return progress**0.3  # Faster than square root"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec428415",
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradient_weighted_loss(x, x_hat, mu, logvar, beta=1.0):\n",
        "    \"\"\"\n",
        "    EXTREME weighting for 99.7% background data\n",
        "    Focus entirely on edges (balls/ground)\n",
        "    \"\"\"\n",
        "    batch_size = x.size(0)\n",
        "\n",
        "    # Sobel edge detection\n",
        "    sobel_x = torch.tensor(\n",
        "        [[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32, device=x.device\n",
        "    ).view(1, 1, 3, 3)\n",
        "    sobel_y = torch.tensor(\n",
        "        [[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32, device=x.device\n",
        "    ).view(1, 1, 3, 3)\n",
        "\n",
        "    x_padded = torch.nn.functional.pad(x, (1, 1, 1, 1), mode='replicate')\n",
        "    grad_x = torch.nn.functional.conv2d(x_padded, sobel_x)\n",
        "    grad_y = torch.nn.functional.conv2d(x_padded, sobel_y)\n",
        "    gradient_magnitude = torch.sqrt(grad_x**2 + grad_y**2)\n",
        "\n",
        "    # Normalize\n",
        "    grad_max = gradient_magnitude.max()\n",
        "    if grad_max > 1e-8:\n",
        "        gradient_magnitude = gradient_magnitude / grad_max\n",
        "\n",
        "    # EXTREME weighting: 1-100x (was 1-21x)\n",
        "    mse = (x_hat - x) ** 2\n",
        "    weights = 0.1 + 100.0 * gradient_magnitude  # Background=0.1, edges=100\n",
        "\n",
        "    recon_loss = (mse * weights).sum() / batch_size\n",
        "\n",
        "    # Free bits KL\n",
        "    kl_per_dim = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    kl_per_dim_clamped = torch.clamp(kl_per_dim - 1.5, min=0.0)  # Lower free bits\n",
        "    kl_loss = kl_per_dim_clamped.sum() / batch_size\n",
        "\n",
        "    actual_kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / batch_size\n",
        "\n",
        "    total_loss = recon_loss + beta * kl_loss\n",
        "    return total_loss, recon_loss, actual_kl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ce80837",
      "metadata": {
        "id": "4ce80837"
      },
      "source": [
        "#### Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eac6e784",
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_validation_samples(\n",
        "    model, test_loader, global_epoch, device, save_dir, phase_name=\"\"\n",
        "):\n",
        "    \"\"\"Save validation set reconstructions\"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    model.eval()\n",
        "    val_images = next(iter(test_loader))[:8].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        val_recon, _ = model(val_images)\n",
        "\n",
        "    comparison = torch.cat([val_images, val_recon], dim=0)\n",
        "\n",
        "    filename = f\"{phase_name}_val_epoch_{global_epoch:03d}.png\"\n",
        "    filepath = os.path.join(save_dir, filename)\n",
        "    torchvision.utils.save_image(comparison, filepath, nrow=8, padding=2)\n",
        "\n",
        "    model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2b0b8eca",
      "metadata": {
        "id": "2b0b8eca"
      },
      "outputs": [],
      "source": [
        "def save_reconstruction_samples(model, images, epoch, batch_idx, save_dir):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        sample_images = images[:4]\n",
        "        reconstructed, _ = model(sample_images)\n",
        "\n",
        "        comparison = torch.cat([sample_images, reconstructed], dim=3)\n",
        "\n",
        "        filepath = os.path.join(\n",
        "            save_dir, f\"epoch_{epoch + 1}_batch_{batch_idx + 1}.png\"\n",
        "        )\n",
        "        torchvision.utils.save_image(comparison, filepath, nrow=1)\n",
        "    model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "063f00fb",
      "metadata": {
        "id": "063f00fb"
      },
      "outputs": [],
      "source": [
        "def validate(model, test_loader, device, beta=1.0):\n",
        "    \"\"\"Validation with weighting\"\"\"\n",
        "    model.eval()\n",
        "    val_loss = val_recon = val_kl = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images in test_loader:\n",
        "            images = images.to(device)\n",
        "            reconstructed, encoded = model(images)\n",
        "            mu, logvar = torch.chunk(encoded, 2, dim=1)\n",
        "\n",
        "            loss, recon_loss, kl_loss = gradient_weighted_loss(\n",
        "                images, reconstructed, mu, logvar, beta=beta\n",
        "            )\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            val_recon += recon_loss.item()\n",
        "            val_kl += kl_loss.item()\n",
        "\n",
        "    model.train()\n",
        "    return (\n",
        "        val_loss / len(test_loader),\n",
        "        val_recon / len(test_loader),\n",
        "        val_kl / len(test_loader),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "16ee5109",
      "metadata": {
        "id": "16ee5109"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(model, optimizer, epoch, loss, filepath, is_best=False):\n",
        "    checkpoint = {\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"loss\": loss,\n",
        "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "    }\n",
        "    torch.save(checkpoint, filepath)\n",
        "\n",
        "    if not is_best:\n",
        "        print(f\"Checkpoint saved: {filepath}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c6fc7285",
      "metadata": {
        "id": "c6fc7285"
      },
      "outputs": [],
      "source": [
        "def plot_training_curves(history, save_dir):\n",
        "    \"\"\"\n",
        "    Plot and save training curves\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "    # Total Loss\n",
        "    axes[0, 0].plot(\n",
        "        epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2\n",
        "    )\n",
        "    axes[0, 0].plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
        "    axes[0, 0].set_title('Total Loss', fontsize=14, fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Reconstruction Loss\n",
        "    axes[0, 1].plot(\n",
        "        epochs, history['train_recon'], 'b-', label='Train Recon', linewidth=2\n",
        "    )\n",
        "    axes[0, 1].plot(epochs, history['val_recon'], 'r-', label='Val Recon', linewidth=2)\n",
        "    axes[0, 1].set_title('Reconstruction Loss', fontsize=14, fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('MSE Loss')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # KL Divergence\n",
        "    axes[1, 0].plot(epochs, history['train_kl'], 'b-', label='Train KL', linewidth=2)\n",
        "    axes[1, 0].plot(epochs, history['val_kl'], 'r-', label='Val KL', linewidth=2)\n",
        "    axes[1, 0].set_title('KL Divergence', fontsize=14, fontweight='bold')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('KL Loss')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Learning Rate\n",
        "    axes[1, 1].plot(epochs, history['learning_rates'], 'g-', linewidth=2)\n",
        "    axes[1, 1].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('Learning Rate')\n",
        "    axes[1, 1].set_yscale('log')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\n",
        "        os.path.join(save_dir, 'training_curves.png'), dpi=150, bbox_inches='tight'\n",
        "    )\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfacd3bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(\n",
        "    model, train_loader, test_loader, device, num_epochs=50, save_dir=\"./saved_models\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Training for 99.7% background data\n",
        "    \"\"\"\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    os.makedirs(\"./reconstructed_samples\", exist_ok=True)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  # Higher LR\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=3\n",
        "    )\n",
        "\n",
        "    history = {\n",
        "        \"train_loss\": [],\n",
        "        \"train_recon\": [],\n",
        "        \"train_kl\": [],\n",
        "        \"val_loss\": [],\n",
        "        \"val_recon\": [],\n",
        "        \"val_kl\": [],\n",
        "        \"learning_rates\": [],\n",
        "        \"betas\": [],\n",
        "    }\n",
        "\n",
        "    avg_train_loss = 0\n",
        "    best_val_loss = float(\"inf\")\n",
        "    num_batches = len(train_loader)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"Training Starting - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Epochs: {num_epochs}\")\n",
        "    print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_recon = 0\n",
        "        train_kl = 0\n",
        "        epoch_beta_sum = 0\n",
        "\n",
        "        for batch_idx, images in enumerate(train_loader):\n",
        "            images = images.to(device)\n",
        "\n",
        "            beta = aggressive_beta(epoch, batch_idx, num_batches, warmup_epochs=8)\n",
        "            epoch_beta_sum += beta\n",
        "\n",
        "            # Forward\n",
        "            recon, encoded = model(images)\n",
        "            mu, logvar = torch.chunk(encoded, 2, dim=1)\n",
        "\n",
        "            # Gradient weighting\n",
        "            loss, recon_loss, kl_loss = gradient_weighted_loss(\n",
        "                images, recon, mu, logvar, beta=beta\n",
        "            )\n",
        "\n",
        "            # Backward\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                model.parameters(), max_norm=2.0\n",
        "            )  # Higher clip\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            train_recon += recon_loss.item()\n",
        "            train_kl += kl_loss.item()\n",
        "\n",
        "            if (batch_idx + 1) % 20 == 0:\n",
        "                print(\n",
        "                    f\"E[{epoch+1:02d}/{num_epochs}] \"\n",
        "                    f\"B[{batch_idx+1:03d}/{num_batches}] | \"\n",
        "                    f\"L:{loss.item():.3f} R:{recon_loss.item():.3f} \"\n",
        "                    f\"KL:{kl_loss.item():.3f} β:{beta:.3f}\"\n",
        "                )\n",
        "\n",
        "                if beta > 0.2 and kl_loss.item() < 1.0:\n",
        "                    print(f\"     KL={kl_loss.item():.2f} still low at β={beta:.2f}\")\n",
        "\n",
        "            if (batch_idx + 1) % 100 == 0:\n",
        "                save_reconstruction_samples(\n",
        "                    model, images, epoch, batch_idx, \"./reconstructed_samples\"\n",
        "                )\n",
        "\n",
        "        # Epoch metrics\n",
        "        avg_train_loss = train_loss / num_batches\n",
        "        avg_train_recon = train_recon / num_batches\n",
        "        avg_train_kl = train_kl / num_batches\n",
        "        avg_beta = epoch_beta_sum / num_batches\n",
        "\n",
        "        # Validation\n",
        "        val_loss, val_recon, val_kl = validate(\n",
        "            model, test_loader, device, beta=avg_beta\n",
        "        )\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "        # Update history\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['train_recon'].append(avg_train_recon)\n",
        "        history['train_kl'].append(avg_train_kl)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_recon'].append(val_recon)\n",
        "        history['val_kl'].append(val_kl)\n",
        "        history['learning_rates'].append(current_lr)\n",
        "        history['betas'].append(avg_beta)\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] Summary:\")\n",
        "        print(f\"{'-'*70}\")\n",
        "        print(\n",
        "            f\"  Train: L={avg_train_loss:.4f} R={avg_train_recon:.4f} KL={avg_train_kl:.4f}\"\n",
        "        )\n",
        "        print(f\"  Val:   L={val_loss:.4f} R={val_recon:.4f} KL={val_kl:.4f}\")\n",
        "        print(f\"  Beta={avg_beta:.3f} LR={current_lr:.2e}\")\n",
        "\n",
        "        if avg_beta >= 0.8:\n",
        "            if avg_train_kl < 1.0:\n",
        "                print(f\"    CRITICAL: KL={avg_train_kl:.2f} < 1.0\")\n",
        "            elif avg_train_kl < 2.0:\n",
        "                print(f\"     WARNING: KL={avg_train_kl:.2f} < 2.0\")\n",
        "            else:\n",
        "                print(f\"    KL healthy: {avg_train_kl:.2f}\")\n",
        "        else:\n",
        "            print(f\"  ℹ  Warmup: β={avg_beta:.2f}\")\n",
        "\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            save_checkpoint(\n",
        "                model,\n",
        "                optimizer,\n",
        "                epoch,\n",
        "                val_loss,\n",
        "                os.path.join(save_dir, 'best_model.pth'),\n",
        "                is_best=True,\n",
        "            )\n",
        "            print(f\"  Best model saved! (Val Loss: {val_loss:.4f})\\n\")\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            save_checkpoint(\n",
        "                model,\n",
        "                optimizer,\n",
        "                epoch,\n",
        "                avg_train_loss,\n",
        "                os.path.join(save_dir, f\"checkpoint_epoch_{epoch+1}.pth\"),\n",
        "            )\n",
        "            save_validation_samples(\n",
        "                model, test_loader, epoch, device, \"./reconstructed_samples\"\n",
        "            )\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    save_checkpoint(\n",
        "        model,\n",
        "        optimizer,\n",
        "        num_epochs - 1,\n",
        "        avg_train_loss,\n",
        "        os.path.join(save_dir, \"final_model.pth\"),\n",
        "    )\n",
        "\n",
        "    print(\"\\n  Training Complete!\")\n",
        "    print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
        "    print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3c6d477",
      "metadata": {
        "id": "c3c6d477"
      },
      "source": [
        "### Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f5036f01",
      "metadata": {
        "id": "f5036f01"
      },
      "outputs": [],
      "source": [
        "class VideoFramesDataset(Dataset):\n",
        "    def __init__(self, frames_dir, transform=None):\n",
        "        # Procura recursivamente por todos os frames .jpg\n",
        "        self.frame_files = sorted(\n",
        "            [\n",
        "                os.path.join(root, f)\n",
        "                for root, _, files in os.walk(frames_dir)\n",
        "                for f in files\n",
        "                if f.lower().endswith(\".jpg\")\n",
        "            ]\n",
        "        )\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.frame_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.frame_files[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cfd1cdf",
      "metadata": {
        "id": "2cfd1cdf"
      },
      "source": [
        "### Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3e8ae5a1",
      "metadata": {
        "id": "3e8ae5a1"
      },
      "outputs": [],
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_channels, out_channels, kernel_size=3, stride=1, padding=1\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
        "        self.activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.activation(self.batch_norm(self.conv(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "f7b654d9",
      "metadata": {
        "id": "f7b654d9"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels) -> None:\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=channels,\n",
        "            out_channels=channels,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "        )\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels=channels,\n",
        "            out_channels=channels,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "        )\n",
        "        self.batch_norm_1 = nn.BatchNorm2d(channels)\n",
        "        self.batch_norm_2 = nn.BatchNorm2d(channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = F.relu(self.batch_norm_1(self.conv1(x)))\n",
        "        out = self.batch_norm_2(self.conv2(out))\n",
        "        return F.relu(out + residual)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3y3JVOwPai8g",
      "metadata": {
        "id": "3y3JVOwPai8g"
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.utils.spectral_norm(nn.Conv2d(1, 32, 3, stride=1, padding=1)),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.utils.spectral_norm(nn.Conv2d(32, 64, 3, stride=2, padding=1)),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            ResidualBlock(64),\n",
        "            nn.utils.spectral_norm(nn.Conv2d(64, 128, 3, stride=2, padding=1)),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            ResidualBlock(128),\n",
        "            nn.utils.spectral_norm(nn.Conv2d(128, 256, 3, stride=2, padding=1)),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            ResidualBlock(256),\n",
        "            nn.utils.spectral_norm(nn.Conv2d(256, 16, kernel_size=1)),\n",
        "        )\n",
        "\n",
        "        self.decoder_main = nn.Sequential(\n",
        "            ConvBlock(8, 64, kernel_size=1, padding=0),\n",
        "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
        "            ConvBlock(64, 32, kernel_size=3, padding=1),\n",
        "            ResidualBlock(32),\n",
        "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
        "            ConvBlock(32, 16, kernel_size=3, padding=1),\n",
        "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "        )\n",
        "\n",
        "        # Simple final layer\n",
        "        self.final_conv = nn.Conv2d(32, 1, kernel_size=3, padding=1)\n",
        "\n",
        "        # Initialize\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        nn.init.xavier_normal_(self.final_conv.weight, gain=0.02)\n",
        "        nn.init.constant_(self.final_conv.bias, 0.0)\n",
        "\n",
        "    def reparametrization(self, mean, log_variance):\n",
        "        std = torch.exp(0.5 * log_variance)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps.mul(std).add_(mean)\n",
        "\n",
        "    def decoder(self, z):\n",
        "        x = self.decoder_main(z)\n",
        "        x = self.final_conv(x)\n",
        "        return torch.clamp(x, 0, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        mean, log_variance = torch.chunk(encoded, 2, dim=1)\n",
        "        log_variance = torch.clamp(log_variance, -10, 10)\n",
        "        z = self.reparametrization(mean, log_variance)\n",
        "        reconstructed = self.decoder(z)\n",
        "        return reconstructed, encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "323b97a4",
      "metadata": {
        "id": "323b97a4"
      },
      "source": [
        "## Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "360adc2b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "360adc2b",
        "outputId": "af663bf9-a00b-43d5-dfd2-8ba5a27b0bb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Extraídos 74 frames (de 366 totais) para frames/random_input_20251005161831_155_auto_05-10-2025_16-19-37\n",
            "✅ Extraídos 88 frames (de 438 totais) para frames/random_input_20251005161831_69_auto_05-10-2025_16-21-50\n",
            "✅ Extraídos 141 frames (de 701 totais) para frames/random_input_20251005161831_124_auto_05-10-2025_16-24-19\n",
            "✅ Extraídos 135 frames (de 672 totais) para frames/random_input_20251005161830_64_auto_05-10-2025_16-23-57\n",
            "✅ Extraídos 137 frames (de 684 totais) para frames/random_input_20251005161831_85_auto_05-10-2025_16-23-02\n",
            "✅ Extraídos 92 frames (de 457 totais) para frames/random_input_20251005161830_12_auto_05-10-2025_16-21-28\n",
            "✅ Extraídos 75 frames (de 371 totais) para frames/random_input_20251005161831_78_auto_05-10-2025_16-21-10\n",
            "✅ Extraídos 140 frames (de 699 totais) para frames/random_input_20251005161830_26_auto_05-10-2025_16-22-53\n",
            "✅ Extraídos 74 frames (de 367 totais) para frames/random_input_20251005161830_17_auto_05-10-2025_16-24-24\n",
            "✅ Extraídos 130 frames (de 648 totais) para frames/random_input_20251005161831_126_auto_05-10-2025_16-24-10\n",
            "✅ Extraídos 134 frames (de 668 totais) para frames/random_input_20251005161831_198_auto_05-10-2025_16-21-41\n",
            "✅ Extraídos 138 frames (de 686 totais) para frames/random_input_20251005161831_115_auto_05-10-2025_16-21-55\n",
            "✅ Extraídos 139 frames (de 692 totais) para frames/random_input_20251005161831_83_auto_05-10-2025_16-23-28\n",
            "✅ Extraídos 144 frames (de 718 totais) para frames/random_input_20251005161831_103_auto_05-10-2025_16-21-13\n",
            "✅ Extraídos 141 frames (de 704 totais) para frames/random_input_20251005161831_73_auto_05-10-2025_16-21-30\n",
            "✅ Extraídos 137 frames (de 682 totais) para frames/random_input_20251005161830_18_auto_05-10-2025_16-22-30\n",
            "✅ Extraídos 135 frames (de 675 totais) para frames/random_input_20251005161830_63_auto_05-10-2025_16-21-44\n",
            "✅ Extraídos 137 frames (de 683 totais) para frames/random_input_20251005161831_105_auto_05-10-2025_16-21-27\n",
            "✅ Extraídos 91 frames (de 452 totais) para frames/random_input_20251005161830_34_auto_05-10-2025_16-19-02\n",
            "✅ Extraídos 119 frames (de 592 totais) para frames/random_input_20251005161831_152_auto_05-10-2025_16-22-10\n",
            "✅ Extraídos 92 frames (de 457 totais) para frames/random_input_20251005161831_122_auto_05-10-2025_16-23-19\n",
            "✅ Extraídos 139 frames (de 695 totais) para frames/random_input_20251005161830_21_auto_05-10-2025_16-19-04\n",
            "✅ Extraídos 137 frames (de 681 totais) para frames/random_input_20251005161831_114_auto_05-10-2025_16-23-38\n",
            "✅ Extraídos 96 frames (de 479 totais) para frames/random_input_20251005161830_61_auto_05-10-2025_16-24-31\n",
            "✅ Extraídos 144 frames (de 720 totais) para frames/random_input_20251005161830_02_auto_05-10-2025_16-19-43\n",
            "✅ Extraídos 142 frames (de 708 totais) para frames/random_input_20251005161830_32_auto_05-10-2025_16-22-49\n",
            "✅ Extraídos 137 frames (de 681 totais) para frames/random_input_20251005161831_142_auto_05-10-2025_16-22-19\n",
            "✅ Extraídos 108 frames (de 538 totais) para frames/random_input_20251005161831_162_auto_05-10-2025_16-20-32\n",
            "✅ Extraídos 143 frames (de 713 totais) para frames/random_input_20251005161831_106_auto_05-10-2025_16-18-40\n",
            "✅ Extraídos 137 frames (de 684 totais) para frames/random_input_20251005161831_109_auto_05-10-2025_16-19-16\n",
            "✅ Extraídos 136 frames (de 679 totais) para frames/random_input_20251005161831_100_auto_05-10-2025_16-18-57\n",
            "✅ Extraídos 100 frames (de 499 totais) para frames/random_input_20251005161831_145_auto_05-10-2025_16-19-25\n",
            "✅ Extraídos 95 frames (de 474 totais) para frames/random_input_20251005161831_111_auto_05-10-2025_16-19-05\n",
            "✅ Extraídos 126 frames (de 630 totais) para frames/random_input_20251005161831_138_auto_05-10-2025_16-18-40\n",
            "✅ Extraídos 129 frames (de 644 totais) para frames/random_input_20251005161831_97_auto_05-10-2025_16-20-11\n",
            "✅ Extraídos 139 frames (de 693 totais) para frames/random_input_20251005161831_81_auto_05-10-2025_16-23-00\n",
            "✅ Extraídos 93 frames (de 465 totais) para frames/random_input_20251005161830_03_auto_05-10-2025_16-23-43\n",
            "✅ Extraídos 143 frames (de 712 totais) para frames/random_input_20251005161831_71_auto_05-10-2025_16-21-45\n",
            "✅ Extraídos 143 frames (de 715 totais) para frames/random_input_20251005161831_82_auto_05-10-2025_16-18-40\n",
            "✅ Extraídos 91 frames (de 455 totais) para frames/random_input_20251005161831_170_auto_05-10-2025_16-21-14\n",
            "✅ Extraídos 123 frames (de 612 totais) para frames/random_input_20251005161831_187_auto_05-10-2025_16-23-19\n",
            "✅ Extraídos 141 frames (de 702 totais) para frames/random_input_20251005161831_79_auto_05-10-2025_16-18-40\n",
            "✅ Extraídos 93 frames (de 461 totais) para frames/random_input_20251005161831_132_auto_05-10-2025_16-24-00\n",
            "✅ Extraídos 143 frames (de 712 totais) para frames/random_input_20251005161831_174_auto_05-10-2025_16-20-49\n",
            "✅ Extraídos 141 frames (de 701 totais) para frames/random_input_20251005161830_27_auto_05-10-2025_16-23-15\n",
            "✅ Extraídos 142 frames (de 709 totais) para frames/random_input_20251005161831_143_auto_05-10-2025_16-19-40\n",
            "✅ Extraídos 132 frames (de 658 totais) para frames/random_input_20251005161831_88_auto_05-10-2025_16-18-40\n",
            "✅ Extraídos 131 frames (de 654 totais) para frames/random_input_20251005161830_44_auto_05-10-2025_16-23-30\n",
            "✅ Extraídos 72 frames (de 358 totais) para frames/random_input_20251005161831_147_auto_05-10-2025_16-23-16\n",
            "✅ Extraídos 95 frames (de 473 totais) para frames/random_input_20251005161831_199_auto_05-10-2025_16-22-48\n",
            "✅ Extraídos 137 frames (de 681 totais) para frames/random_input_20251005161830_31_auto_05-10-2025_16-22-57\n",
            "✅ Extraídos 135 frames (de 674 totais) para frames/random_input_20251005161831_107_auto_05-10-2025_16-21-02\n",
            "✅ Extraídos 143 frames (de 714 totais) para frames/random_input_20251005161830_35_auto_05-10-2025_16-21-44\n",
            "✅ Extraídos 128 frames (de 640 totais) para frames/random_input_20251005161831_135_auto_05-10-2025_16-20-59\n",
            "✅ Extraídos 143 frames (de 711 totais) para frames/random_input_20251005161831_148_auto_05-10-2025_16-23-08\n",
            "✅ Extraídos 126 frames (de 630 totais) para frames/random_input_20251005161831_99_auto_05-10-2025_16-19-16\n",
            "✅ Extraídos 133 frames (de 661 totais) para frames/random_input_20251005161831_93_auto_05-10-2025_16-20-02\n",
            "✅ Extraídos 144 frames (de 717 totais) para frames/random_input_20251005161831_173_auto_05-10-2025_16-21-29\n",
            "✅ Extraídos 71 frames (de 354 totais) para frames/random_input_20251005161831_197_auto_05-10-2025_16-21-37\n",
            "✅ Extraídos 140 frames (de 698 totais) para frames/random_input_20251005161831_193_auto_05-10-2025_16-21-14\n",
            "✅ Extraídos 144 frames (de 718 totais) para frames/random_input_20251005161830_57_auto_05-10-2025_16-20-23\n",
            "✅ Extraídos 139 frames (de 691 totais) para frames/random_input_20251005161830_48_auto_05-10-2025_16-18-58\n",
            "✅ Extraídos 112 frames (de 557 totais) para frames/random_input_20251005161830_14_auto_05-10-2025_16-20-19\n",
            "✅ Extraídos 85 frames (de 423 totais) para frames/random_input_20251005161830_15_auto_05-10-2025_16-23-41\n",
            "✅ Extraídos 142 frames (de 710 totais) para frames/random_input_20251005161831_137_auto_05-10-2025_16-20-41\n",
            "✅ Extraídos 141 frames (de 703 totais) para frames/random_input_20251005161830_06_auto_05-10-2025_16-21-04\n",
            "✅ Extraídos 140 frames (de 700 totais) para frames/random_input_20251005161831_158_auto_05-10-2025_16-19-48\n",
            "✅ Extraídos 110 frames (de 547 totais) para frames/random_input_20251005161831_91_auto_05-10-2025_16-23-27\n",
            "✅ Extraídos 113 frames (de 561 totais) para frames/random_input_20251005161831_130_auto_05-10-2025_16-22-54\n",
            "✅ Extraídos 143 frames (de 715 totais) para frames/random_input_20251005161831_120_auto_05-10-2025_16-19-02\n",
            "✅ Extraídos 136 frames (de 677 totais) para frames/random_input_20251005161831_171_auto_05-10-2025_16-20-32\n",
            "✅ Extraídos 99 frames (de 495 totais) para frames/random_input_20251005161830_62_auto_05-10-2025_16-24-09\n",
            "✅ Extraídos 73 frames (de 363 totais) para frames/random_input_20251005161831_195_auto_05-10-2025_16-20-04\n",
            "✅ Extraídos 134 frames (de 666 totais) para frames/random_input_20251005161830_07_auto_05-10-2025_16-22-34\n",
            "✅ Extraídos 131 frames (de 652 totais) para frames/random_input_20251005161831_150_auto_05-10-2025_16-21-49\n",
            "✅ Extraídos 141 frames (de 701 totais) para frames/random_input_20251005161830_08_auto_05-10-2025_16-18-54\n",
            "✅ Extraídos 109 frames (de 544 totais) para frames/random_input_20251005161830_20_auto_05-10-2025_16-18-40\n",
            "✅ Extraídos 142 frames (de 709 totais) para frames/random_input_20251005161831_70_auto_05-10-2025_16-19-38\n",
            "✅ Extraídos 101 frames (de 503 totais) para frames/random_input_20251005161831_76_auto_05-10-2025_16-20-30\n",
            "✅ Extraídos 128 frames (de 640 totais) para frames/random_input_20251005161831_128_auto_05-10-2025_16-23-15\n",
            "✅ Extraídos 133 frames (de 661 totais) para frames/random_input_20251005161830_33_auto_05-10-2025_16-19-52\n",
            "✅ Extraídos 101 frames (de 501 totais) para frames/random_input_20251005161830_10_auto_05-10-2025_16-19-25\n",
            "✅ Extraídos 101 frames (de 501 totais) para frames/random_input_20251005161830_58_auto_05-10-2025_16-22-16\n",
            "✅ Extraídos 75 frames (de 372 totais) para frames/random_input_20251005161830_28_auto_05-10-2025_16-20-21\n",
            "✅ Extraídos 100 frames (de 500 totais) para frames/random_input_20251005161830_49_auto_05-10-2025_16-18-40\n",
            "✅ Extraídos 141 frames (de 703 totais) para frames/random_input_20251005161831_182_auto_05-10-2025_16-23-30\n",
            "✅ Extraídos 78 frames (de 389 totais) para frames/random_input_20251005161831_87_auto_05-10-2025_16-22-06\n",
            "✅ Extraídos 140 frames (de 698 totais) para frames/random_input_20251005161831_178_auto_05-10-2025_16-20-50\n",
            "✅ Extraídos 94 frames (de 470 totais) para frames/random_input_20251005161831_68_auto_05-10-2025_16-19-36\n",
            "✅ Extraídos 144 frames (de 720 totais) para frames/random_input_20251005161831_172_auto_05-10-2025_16-20-47\n",
            "✅ Extraídos 78 frames (de 387 totais) para frames/random_input_20251005161831_163_auto_05-10-2025_16-23-14\n",
            "✅ Extraídos 85 frames (de 422 totais) para frames/random_input_20251005161831_66_auto_05-10-2025_16-20-26\n",
            "✅ Extraídos 88 frames (de 440 totais) para frames/random_input_20251005161831_159_auto_05-10-2025_16-20-01\n",
            "✅ Extraídos 90 frames (de 448 totais) para frames/random_input_20251005161831_160_auto_05-10-2025_16-23-26\n",
            "✅ Extraídos 122 frames (de 606 totais) para frames/random_input_20251005161831_113_auto_05-10-2025_16-23-56\n",
            "✅ Extraídos 141 frames (de 702 totais) para frames/random_input_20251005161831_190_auto_05-10-2025_16-18-54\n",
            "✅ Extraídos 144 frames (de 716 totais) para frames/random_input_20251005161830_38_auto_05-10-2025_16-24-22\n",
            "✅ Extraídos 144 frames (de 719 totais) para frames/random_input_20251005161831_94_auto_05-10-2025_16-23-41\n",
            "✅ Extraídos 129 frames (de 641 totais) para frames/random_input_20251005161831_140_auto_05-10-2025_16-21-37\n",
            "✅ Extraídos 88 frames (de 440 totais) para frames/random_input_20251005161831_176_auto_05-10-2025_16-22-29\n",
            "✅ Extraídos 119 frames (de 595 totais) para frames/random_input_20251005161830_24_auto_05-10-2025_16-18-40\n",
            "✅ Extraídos 138 frames (de 688 totais) para frames/random_input_20251005161830_55_auto_05-10-2025_16-19-49\n",
            "✅ Extraídos 136 frames (de 679 totais) para frames/random_input_20251005161830_50_auto_05-10-2025_16-20-16\n",
            "✅ Extraídos 103 frames (de 513 totais) para frames/random_input_20251005161830_41_auto_05-10-2025_16-19-17\n",
            "✅ Extraídos 133 frames (de 665 totais) para frames/random_input_20251005161831_84_auto_05-10-2025_16-24-17\n",
            "✅ Extraídos 91 frames (de 455 totais) para frames/random_input_20251005161831_127_auto_05-10-2025_16-23-36\n",
            "✅ Extraídos 106 frames (de 526 totais) para frames/random_input_20251005161830_37_auto_05-10-2025_16-20-24\n",
            "✅ Extraídos 70 frames (de 349 totais) para frames/random_input_20251005161831_74_auto_05-10-2025_16-21-18\n",
            "✅ Extraídos 78 frames (de 390 totais) para frames/random_input_20251005161831_164_auto_05-10-2025_16-19-05\n",
            "✅ Extraídos 70 frames (de 348 totais) para frames/random_input_20251005161831_134_auto_05-10-2025_16-21-54\n",
            "✅ Extraídos 110 frames (de 549 totais) para frames/random_input_20251005161831_194_auto_05-10-2025_16-22-30\n",
            "✅ Extraídos 66 frames (de 326 totais) para frames/random_input_20251005161830_19_auto_05-10-2025_16-22-06\n",
            "✅ Extraídos 134 frames (de 667 totais) para frames/random_input_20251005161831_151_auto_05-10-2025_16-21-22\n",
            "✅ Extraídos 143 frames (de 713 totais) para frames/random_input_20251005161831_188_auto_05-10-2025_16-19-20\n",
            "✅ Extraídos 138 frames (de 690 totais) para frames/random_input_20251005161831_167_auto_05-10-2025_16-19-41\n",
            "✅ Extraídos 134 frames (de 666 totais) para frames/random_input_20251005161830_46_auto_05-10-2025_16-20-03\n",
            "✅ Extraídos 90 frames (de 450 totais) para frames/random_input_20251005161830_51_auto_05-10-2025_16-24-35\n",
            "✅ Extraídos 115 frames (de 573 totais) para frames/random_input_20251005161830_05_auto_05-10-2025_16-24-13\n",
            "✅ Extraídos 80 frames (de 397 totais) para frames/random_input_20251005161830_42_auto_05-10-2025_16-22-00\n",
            "✅ Extraídos 72 frames (de 357 totais) para frames/random_input_20251005161830_36_auto_05-10-2025_16-23-05\n",
            "✅ Extraídos 87 frames (de 435 totais) para frames/random_input_20251005161830_22_auto_05-10-2025_16-18-40\n",
            "✅ Extraídos 121 frames (de 603 totais) para frames/random_input_20251005161830_09_auto_05-10-2025_16-20-24\n",
            "✅ Extraídos 143 frames (de 711 totais) para frames/random_input_20251005161831_104_auto_05-10-2025_16-22-09\n",
            "✅ Extraídos 130 frames (de 647 totais) para frames/random_input_20251005161831_133_auto_05-10-2025_16-18-40\n",
            "✅ Extraídos 141 frames (de 705 totais) para frames/random_input_20251005161831_168_auto_05-10-2025_16-21-51\n",
            "✅ Extraídos 133 frames (de 662 totais) para frames/random_input_20251005161830_56_auto_05-10-2025_16-21-07\n",
            "✅ Extraídos 144 frames (de 720 totais) para frames/random_input_20251005161831_146_auto_05-10-2025_16-22-57\n",
            "✅ Extraídos 134 frames (de 667 totais) para frames/random_input_20251005161831_139_auto_05-10-2025_16-20-28\n",
            "✅ Extraídos 138 frames (de 690 totais) para frames/random_input_20251005161830_13_auto_05-10-2025_16-23-23\n",
            "✅ Extraídos 142 frames (de 709 totais) para frames/random_input_20251005161831_180_auto_05-10-2025_16-22-48\n",
            "✅ Extraídos 144 frames (de 720 totais) para frames/random_input_20251005161830_16_auto_05-10-2025_16-21-45\n",
            "✅ Extraídos 105 frames (de 523 totais) para frames/random_input_20251005161830_52_auto_05-10-2025_16-22-10\n",
            "✅ Extraídos 138 frames (de 687 totais) para frames/random_input_20251005161831_98_auto_05-10-2025_16-23-05\n",
            "✅ Extraídos 128 frames (de 638 totais) para frames/random_input_20251005161831_123_auto_05-10-2025_16-22-43\n",
            "✅ Extraídos 144 frames (de 720 totais) para frames/random_input_20251005161830_29_auto_05-10-2025_16-22-37\n",
            "✅ Extraídos 133 frames (de 663 totais) para frames/random_input_20251005161830_39_auto_05-10-2025_16-20-02\n",
            "✅ Extraídos 79 frames (de 391 totais) para frames/random_input_20251005161830_54_auto_05-10-2025_16-20-11\n",
            "✅ Extraídos 136 frames (de 676 totais) para frames/random_input_20251005161830_30_auto_05-10-2025_16-19-40\n",
            "✅ Extraídos 83 frames (de 414 totais) para frames/random_input_20251005161831_186_auto_05-10-2025_16-22-15\n",
            "✅ Extraídos 137 frames (de 681 totais) para frames/random_input_20251005161831_169_auto_05-10-2025_16-23-22\n",
            "✅ Extraídos 117 frames (de 584 totais) para frames/random_input_20251005161831_75_auto_05-10-2025_16-22-08\n",
            "✅ Extraídos 72 frames (de 356 totais) para frames/random_input_20251005161831_86_auto_05-10-2025_16-18-40\n",
            "✅ Extraídos 143 frames (de 711 totais) para frames/random_input_20251005161830_47_auto_05-10-2025_16-21-18\n",
            "✅ Extraídos 104 frames (de 517 totais) para frames/random_input_20251005161831_191_auto_05-10-2025_16-20-42\n",
            "✅ Extraídos 138 frames (de 688 totais) para frames/random_input_20251005161831_136_auto_05-10-2025_16-22-44\n",
            "✅ Extraídos 138 frames (de 688 totais) para frames/random_input_20251005161831_92_auto_05-10-2025_16-18-59\n",
            "✅ Extraídos 131 frames (de 655 totais) para frames/random_input_20251005161831_200_auto_05-10-2025_16-19-18\n",
            "✅ Extraídos 92 frames (de 460 totais) para frames/random_input_20251005161831_166_auto_05-10-2025_16-19-33\n",
            "✅ Extraídos 115 frames (de 574 totais) para frames/random_input_20251005161831_154_auto_05-10-2025_16-20-03\n",
            "✅ Extraídos 140 frames (de 697 totais) para frames/random_input_20251005161830_53_auto_05-10-2025_16-19-39\n",
            "✅ Extraídos 94 frames (de 467 totais) para frames/random_input_20251005161831_102_auto_05-10-2025_16-21-11\n",
            "✅ Extraídos 140 frames (de 700 totais) para frames/random_input_20251005161831_67_auto_05-10-2025_16-21-29\n",
            "✅ Extraídos 138 frames (de 687 totais) para frames/random_input_20251005161831_181_auto_05-10-2025_16-20-47\n",
            "✅ Extraídos 66 frames (de 329 totais) para frames/random_input_20251005161831_112_auto_05-10-2025_16-19-06\n",
            "✅ Extraídos 142 frames (de 706 totais) para frames/random_input_20251005161831_118_auto_05-10-2025_16-20-39\n",
            "✅ Extraídos 136 frames (de 678 totais) para frames/random_input_20251005161831_65_auto_05-10-2025_16-20-38\n",
            "✅ Extraídos 86 frames (de 427 totais) para frames/random_input_20251005161831_72_auto_05-10-2025_16-20-32\n",
            "✅ Extraídos 65 frames (de 325 totais) para frames/random_input_20251005161831_175_auto_05-10-2025_16-22-19\n",
            "✅ Extraídos 79 frames (de 394 totais) para frames/random_input_20251005161831_184_auto_05-10-2025_16-22-33\n",
            "✅ Extraídos 136 frames (de 679 totais) para frames/random_input_20251005161831_144_auto_05-10-2025_16-19-49\n",
            "✅ Extraídos 140 frames (de 700 totais) para frames/random_input_20251005161830_01_auto_05-10-2025_16-22-32\n",
            "✅ Extraídos 63 frames (de 312 totais) para frames/random_input_20251005161831_189_auto_05-10-2025_16-19-48\n",
            "✅ Extraídos 131 frames (de 651 totais) para frames/random_input_20251005161831_89_auto_05-10-2025_16-19-40\n",
            "✅ Extraídos 135 frames (de 671 totais) para frames/random_input_20251005161831_80_auto_05-10-2025_16-24-00\n",
            "✅ Extraídos 124 frames (de 620 totais) para frames/random_input_20251005161830_45_auto_05-10-2025_16-19-58\n",
            "✅ Extraídos 143 frames (de 712 totais) para frames/random_input_20251005161831_192_auto_05-10-2025_16-20-54\n",
            "✅ Extraídos 97 frames (de 485 totais) para frames/random_input_20251005161831_108_auto_05-10-2025_16-22-31\n",
            "✅ Extraídos 114 frames (de 567 totais) para frames/random_input_20251005161831_101_auto_05-10-2025_16-22-56\n",
            "✅ Extraídos 74 frames (de 366 totais) para frames/random_input_20251005161830_59_auto_05-10-2025_16-20-46\n",
            "✅ Extraídos 133 frames (de 664 totais) para frames/random_input_20251005161830_11_auto_05-10-2025_16-22-32\n",
            "✅ Extraídos 107 frames (de 535 totais) para frames/random_input_20251005161830_40_auto_05-10-2025_16-20-59\n",
            "✅ Extraídos 95 frames (de 473 totais) para frames/random_input_20251005161831_116_auto_05-10-2025_16-20-16\n",
            "✅ Extraídos 112 frames (de 558 totais) para frames/random_input_20251005161830_25_auto_05-10-2025_16-19-18\n",
            "✅ Extraídos 99 frames (de 491 totais) para frames/random_input_20251005161831_161_auto_05-10-2025_16-20-11\n",
            "✅ Extraídos 98 frames (de 486 totais) para frames/random_input_20251005161831_119_auto_05-10-2025_16-20-58\n",
            "✅ Extraídos 102 frames (de 507 totais) para frames/random_input_20251005161831_95_auto_05-10-2025_16-22-42\n",
            "✅ Extraídos 144 frames (de 717 totais) para frames/random_input_20251005161831_156_auto_05-10-2025_16-19-25\n",
            "✅ Extraídos 134 frames (de 670 totais) para frames/random_input_20251005161831_96_auto_05-10-2025_16-20-44\n",
            "✅ Extraídos 140 frames (de 696 totais) para frames/random_input_20251005161831_129_auto_05-10-2025_16-23-34\n",
            "✅ Extraídos 139 frames (de 694 totais) para frames/random_input_20251005161831_179_auto_05-10-2025_16-22-05\n",
            "✅ Extraídos 132 frames (de 658 totais) para frames/random_input_20251005161831_77_auto_05-10-2025_16-21-22\n",
            "✅ Extraídos 136 frames (de 679 totais) para frames/random_input_20251005161831_177_auto_05-10-2025_16-21-55\n",
            "✅ Extraídos 70 frames (de 350 totais) para frames/random_input_20251005161831_157_auto_05-10-2025_16-23-54\n",
            "✅ Extraídos 132 frames (de 658 totais) para frames/random_input_20251005161831_121_auto_05-10-2025_16-19-04\n",
            "✅ Extraídos 121 frames (de 603 totais) para frames/random_input_20251005161830_43_auto_05-10-2025_16-19-21\n",
            "✅ Extraídos 129 frames (de 643 totais) para frames/random_input_20251005161831_131_auto_05-10-2025_16-21-00\n",
            "✅ Extraídos 139 frames (de 693 totais) para frames/random_input_20251005161831_110_auto_05-10-2025_16-19-16\n",
            "✅ Extraídos 96 frames (de 477 totais) para frames/random_input_20251005161831_149_auto_05-10-2025_16-24-07\n",
            "✅ Extraídos 83 frames (de 411 totais) para frames/random_input_20251005161830_60_auto_05-10-2025_16-18-40\n",
            "✅ Extraídos 144 frames (de 717 totais) para frames/random_input_20251005161831_185_auto_05-10-2025_16-20-35\n",
            "✅ Extraídos 89 frames (de 443 totais) para frames/random_input_20251005161830_04_auto_05-10-2025_16-22-20\n",
            "✅ Extraídos 139 frames (de 694 totais) para frames/random_input_20251005161831_196_auto_05-10-2025_16-22-17\n",
            "✅ Extraídos 105 frames (de 523 totais) para frames/random_input_20251005161831_153_auto_05-10-2025_16-20-14\n",
            "✅ Extraídos 135 frames (de 675 totais) para frames/random_input_20251005161831_141_auto_05-10-2025_16-22-29\n",
            "✅ Extraídos 78 frames (de 390 totais) para frames/random_input_20251005161831_125_auto_05-10-2025_16-19-35\n",
            "✅ Extraídos 141 frames (de 704 totais) para frames/random_input_20251005161831_183_auto_05-10-2025_16-20-07\n",
            "✅ Extraídos 137 frames (de 684 totais) para frames/random_input_20251005161831_165_auto_05-10-2025_16-21-25\n",
            "✅ Extraídos 105 frames (de 522 totais) para frames/random_input_20251005161831_117_auto_05-10-2025_16-22-13\n",
            "✅ Extraídos 97 frames (de 481 totais) para frames/random_input_20251005161831_90_auto_05-10-2025_16-19-20\n",
            "✅ Extraídos 138 frames (de 689 totais) para frames/random_input_20251005161830_23_auto_05-10-2025_16-21-21\n",
            "train_dataset: 18936 frames\n",
            "test_dataset: 4735 frames\n",
            "device: cuda\n"
          ]
        }
      ],
      "source": [
        "videos_folder = \"dataset/scenario1/videos\"  # Folder with videos\n",
        "frames_root = \"frames\"\n",
        "frame_step = 5\n",
        "\n",
        "# Extracts frames from all videos\n",
        "process_videos(videos_folder, frames_root, frame_step)\n",
        "\n",
        "# Transforming for greyscale images\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((64, 64)),\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Creates dataset from frames\n",
        "full_dataset = VideoFramesDataset(frames_root, transform=transform)\n",
        "\n",
        "# Splits data into training and testing\n",
        "total_size = len(full_dataset)\n",
        "train_size = int(0.8 * total_size)\n",
        "test_size = total_size - train_size\n",
        "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
        "\n",
        "# Creates dataloaders\n",
        "batch_size = 256\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Defines device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f\"train_dataset: {len(train_dataset)} frames\")\n",
        "print(f\"test_dataset: {len(test_dataset)} frames\")\n",
        "print(f\"device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4f3f3eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4f3f3eb",
        "outputId": "dea775de-0277-4c4d-b2fc-d69d6c3e0f02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "EXTREME Training Config - 2025-12-15 18:06:30\n",
            "======================================================================\n",
            "Device: cuda\n",
            "Learning Rate: 1e-3 (higher)\n",
            "Epochs: 30\n",
            "Beta warmup: 8 epochs (ultra-fast)\n",
            "Free bits: 1.5 nats (lower)\n",
            "Edge weighting: 100x (extreme)\n",
            "Data: 99.7% background → needs extreme measures\n",
            "======================================================================\n",
            "\n",
            "E[01/30] B[020/592] | L:367.518 R:347.787 KL:398.748 β:0.191\n",
            "E[01/30] B[040/592] | L:285.206 R:282.117 KL:280.066 β:0.237\n",
            "E[01/30] B[060/592] | L:176.131 R:173.069 KL:327.185 β:0.268\n",
            "E[01/30] B[080/592] | L:166.033 R:163.791 KL:331.492 β:0.293\n",
            "E[01/30] B[100/592] | L:117.708 R:114.460 KL:325.409 β:0.313\n",
            "E[01/30] B[120/592] | L:130.008 R:127.743 KL:367.199 β:0.331\n",
            "E[01/30] B[140/592] | L:176.609 R:172.591 KL:364.283 β:0.347\n",
            "E[01/30] B[160/592] | L:86.407 R:82.477 KL:405.753 β:0.361\n",
            "E[01/30] B[180/592] | L:122.916 R:120.368 KL:387.934 β:0.374\n",
            "E[01/30] B[200/592] | L:91.014 R:87.835 KL:417.000 β:0.386\n",
            "E[01/30] B[220/592] | L:80.944 R:77.113 KL:431.411 β:0.398\n",
            "E[01/30] B[240/592] | L:89.470 R:85.824 KL:420.664 β:0.408\n",
            "E[01/30] B[260/592] | L:77.334 R:73.613 KL:446.996 β:0.418\n",
            "E[01/30] B[280/592] | L:66.921 R:63.899 KL:467.188 β:0.428\n",
            "E[01/30] B[300/592] | L:57.191 R:54.974 KL:459.187 β:0.437\n",
            "E[01/30] B[320/592] | L:58.781 R:56.689 KL:463.026 β:0.445\n",
            "E[01/30] B[340/592] | L:60.857 R:59.627 KL:450.283 β:0.453\n",
            "E[01/30] B[360/592] | L:62.765 R:59.966 KL:449.790 β:0.461\n",
            "E[01/30] B[380/592] | L:52.982 R:50.179 KL:457.029 β:0.469\n",
            "E[01/30] B[400/592] | L:43.724 R:40.284 KL:462.745 β:0.476\n",
            "E[01/30] B[420/592] | L:49.159 R:47.416 KL:465.389 β:0.483\n",
            "E[01/30] B[440/592] | L:91.739 R:88.469 KL:436.966 β:0.490\n",
            "E[01/30] B[460/592] | L:55.341 R:52.365 KL:481.046 β:0.497\n",
            "E[01/30] B[480/592] | L:42.107 R:38.677 KL:452.562 β:0.503\n",
            "E[01/30] B[500/592] | L:57.646 R:55.432 KL:461.820 β:0.509\n",
            "E[01/30] B[520/592] | L:38.636 R:36.325 KL:470.037 β:0.515\n",
            "E[01/30] B[540/592] | L:36.071 R:34.040 KL:486.558 β:0.521\n",
            "E[01/30] B[560/592] | L:40.756 R:38.881 KL:484.854 β:0.527\n",
            "E[01/30] B[580/592] | L:46.119 R:43.100 KL:454.944 β:0.532\n",
            "\n",
            "======================================================================\n",
            "Epoch [1/30] Summary:\n",
            "----------------------------------------------------------------------\n",
            "  Train: L=154.5208 R=149.7132 KL=438.5495\n",
            "  Val:   L=53.7244 R=52.0469 KL=442.8204\n",
            "  Beta=0.412 LR=1.00e-03\n",
            "  ℹ️  Warmup: β=0.41\n",
            "======================================================================\n",
            "\n",
            "✅ Best model saved! (Val Loss: 53.7244)\n",
            "\n",
            "E[02/30] B[020/592] | L:56.992 R:55.133 KL:430.519 β:0.541\n",
            "E[02/30] B[040/592] | L:31.321 R:29.217 KL:442.865 β:0.546\n",
            "E[02/30] B[060/592] | L:37.184 R:35.299 KL:414.400 β:0.551\n",
            "E[02/30] B[080/592] | L:69.320 R:67.769 KL:426.016 β:0.556\n",
            "E[02/30] B[100/592] | L:38.266 R:35.340 KL:442.161 β:0.561\n",
            "E[02/30] B[120/592] | L:37.725 R:34.932 KL:460.523 β:0.566\n",
            "E[02/30] B[140/592] | L:34.943 R:32.385 KL:449.038 β:0.571\n",
            "E[02/30] B[160/592] | L:41.429 R:40.019 KL:450.741 β:0.576\n",
            "E[02/30] B[180/592] | L:41.784 R:39.500 KL:449.470 β:0.580\n",
            "E[02/30] B[200/592] | L:41.062 R:39.761 KL:440.062 β:0.585\n",
            "E[02/30] B[220/592] | L:30.272 R:28.668 KL:441.876 β:0.589\n"
          ]
        }
      ],
      "source": [
        "model = VAE().to(device)\n",
        "\n",
        "# history = train(model=model, train_loader=train_loader, test_loader=test_loader)\n",
        "history = train(model, train_loader, test_loader, device, 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbeb8396",
      "metadata": {},
      "source": [
        "```\n",
        "conv block 1 torch.Size([32, 64, 8, 8])\n",
        "upsample 1 torch.Size([32, 64, 16, 16])\n",
        "conv block 2 torch.Size([32, 32, 16, 16])\n",
        "upsample 2 torch.Size([32, 32, 32, 32])\n",
        "conv block torch.Size([32, 16, 32, 32])\n",
        "upsample 3 torch.Size([32, 16, 64, 64])\n",
        "conv2d block 1 torch.Size([32, 32, 64, 64])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u21kHMNN9yNg",
      "metadata": {
        "id": "u21kHMNN9yNg"
      },
      "outputs": [],
      "source": [
        "!rm -rf reconstructed_samples/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "f2e2ee68",
        "c3c6d477",
        "2cfd1cdf"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
